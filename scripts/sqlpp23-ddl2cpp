#!/usr/bin/env python3

##
# Copyright (c) 2013, Roland Bock
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without modification,
# are permitted provided that the following conditions are met:
#
#  * Redistributions of source code must retain the above copyright notice,
#    this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright notice,
#    this list of conditions and the following disclaimer in the documentation
#    and/or other materials provided with the distribution.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
# IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,
# INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
# LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
# OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
# OF THE POSSIBILITY OF SUCH DAMAGE.
##

import argparse
import pyparsing as pp
import sys
import re
import os
from enum import IntEnum


class ExitCode(IntEnum):
    SUCCESS = 0
    BAD_ARGS = 1
    BAD_DATA_TYPE = 10
    STRANGE_PARSING = 20


# DdlParser, SelfTest and ModelWriter could be modules instead of classes. However, using
# modules implies that the program code would be split into several files and we want to avoid that
# because it would complicate program installation. That is why we use classes.


class DdlParser:
    """Rather crude SQL expression parser.
    This is not geared at correctly interpreting SQL, but at identifying (and ignoring) expressions for instance in DEFAULT expressions
    """

    # Names of data types that can be updated by the custom data types
    ddl_boolean_types = [
        "bool",
        "boolean",
    ]
    ddl_integer_types = [
        "bigint",
        "int",
        "int2",  # PostgreSQL
        "int4",  # PostgreSQL
        "int8",  # PostgreSQL
        "integer",
        "mediumint",
        "smallint",
        "tinyint",
    ]
    ddl_serial_types = [
        "bigserial",  # PostgreSQL
        "serial",  # PostgreSQL
        "serial2",  # PostgreSQL
        "serial4",  # PostgreSQL
        "serial8",  # PostgreSQL
        "smallserial",  # PostgreSQL
    ]
    ddl_floating_point_types = [
        "decimal",  # MYSQL
        "double",
        "float8",  # PostgreSQL
        "float",
        "float4",  # PostgreSQL
        "numeric",  # PostgreSQL
        "real",
    ]
    ddl_text_types = [
        "char",
        "varchar",
        "character varying",  # PostgreSQL
        "text",
        "clob",
        "enum",  # MYSQL
        "set",
        "longtext",  # MYSQL
        "jsonb",  # PostgreSQL
        "json",  # PostgreSQL
        "tinytext",  # MYSQL
        "mediumtext",  # MYSQL
        "rational", # PostgreSQL pg_rationale extension
    ]
    ddl_blob_types = [
        "bytea",
        "tinyblob",
        "blob",
        "mediumblob",
        "longblob",
        "binary",  # MYSQL
        "varbinary",  # MYSQL
    ]
    ddl_date_types = [
        "date",
    ]
    ddl_date_time_types = [
        "datetime",
        "timestamp",
        "timestamp without time zone",  # PostgreSQL
        "timestamp with time zone",  # PostgreSQL
        "timestamptz",  # PostgreSQL
    ]
    ddl_time_types = [
        "time",
        "time without time zone",  # PostgreSQL
        "time with time zone",  # PostgreSQL
    ]

    # Parsers that are initialized later
    ddl_expression = None
    ddl_type = None
    ddl_column = None
    ddl_constraint = None
    ddl_ct_with_sql = None
    ddl = None

    @classmethod
    def initialize(cls, custom_types=None):
        """Initialize the DDL parser"""

        # Basic parsers
        ddl_left = pp.Suppress("(")
        ddl_right = pp.Suppress(")")
        ddl_number = pp.Word(pp.nums + "+-.", pp.nums + "+-.Ee")
        ddl_string = (
            pp.QuotedString("'") | pp.QuotedString('"', esc_quote='""') | pp.QuotedString("`")
        )
        # ddl_string.set_debug(True) #uncomment to debug pyparsing
        ddl_term = pp.Word(pp.alphas + "_", pp.alphanums + "_.$")
        ddl_name = pp.Or([ddl_term, ddl_string, pp.Combine(ddl_string + "." + ddl_string), pp.Combine(ddl_term + ddl_string)])
        ddl_operator = pp.Or(
            map(pp.CaselessLiteral, ["+", "-", "*", "/", "<", "<=", ">", ">=", "=", "%"]),
            pp.CaselessKeyword("DIV")
        )
        ddl_braced_expression = pp.Forward()
        ddl_function_call = pp.Forward()
        ddl_cast_end = "::" + ddl_term
        ddl_cast = ddl_string + ddl_cast_end
        ddl_braced_arguments = pp.Forward()
        cls.ddl_expression = pp.OneOrMore(
            ddl_braced_expression
            | ddl_function_call
            | ddl_cast_end
            | ddl_cast
            | ddl_operator
            | ddl_string
            | ddl_term
            | ddl_number
            | ddl_braced_arguments
        )
        ddl_braced_arguments << ddl_left + pp.DelimitedList(cls.ddl_expression) + ddl_right
        ddl_braced_expression << ddl_left + cls.ddl_expression + ddl_right
        ddl_arguments = pp.Suppress(pp.DelimitedList(cls.ddl_expression))
        ddl_function_call << ddl_name + ddl_left + pp.Optional(ddl_arguments) + ddl_right

        # Data type parsers
        def get_type_parser(key, data_type):
            type_names = getattr(cls, f"ddl_{key}_types")
            if custom_types and (key in custom_types):
                type_names.extend(custom_types[key])
            return pp.Or(
                map(pp.CaselessKeyword, sorted(type_names, reverse=True))
            ).set_parse_action(pp.replace_with(data_type))

        ddl_boolean = get_type_parser("boolean", "boolean")
        ddl_integer = get_type_parser("integer", "integral")
        ddl_serial = get_type_parser("serial", "integral").set_results_name("has_serial_value")
        ddl_floating_point = get_type_parser("floating_point", "floating_point")
        ddl_text = get_type_parser("text", "text")
        ddl_blob = get_type_parser("blob", "blob")
        ddl_date = get_type_parser("date", "date")
        ddl_date_time = get_type_parser("date_time", "timestamp")
        ddl_time = get_type_parser("time", "time")
        ddl_unknown = pp.Word(pp.alphanums).set_parse_action(pp.replace_with("UNKNOWN"))
        cls.ddl_type = (
            ddl_boolean
            | ddl_integer
            | ddl_serial
            | ddl_floating_point
            | ddl_text
            | ddl_blob
            | ddl_date_time
            | ddl_date
            | ddl_time
            | ddl_unknown
        )

        # Constraints parser
        ddl_unsigned = pp.CaselessKeyword("UNSIGNED").set_results_name("is_unsigned")
        ddl_digits = "," + pp.Word(pp.nums)
        ddl_width = ddl_left + pp.Word(pp.nums) + pp.Optional(ddl_digits) + ddl_right
        ddl_timezone = (
            (pp.CaselessKeyword("with") | pp.CaselessKeyword("without"))
            + pp.CaselessKeyword("time")
            + pp.CaselessKeyword("zone")
        )
        ddl_not_null = (pp.CaselessKeyword("NOT") + pp.CaselessKeyword("NULL")).set_results_name("not_null")
        ddl_default_value = pp.CaselessKeyword("DEFAULT").set_results_name("has_default_value")
        ddl_generated_value = pp.CaselessKeyword("GENERATED").set_results_name("has_generated_value")
        ddl_auto_keywords = [
            "AUTO_INCREMENT",
            "AUTOINCREMENT"
        ]
        ddl_auto_value = pp.Or(map(pp.CaselessKeyword, sorted(ddl_auto_keywords, reverse=True))).set_results_name("has_auto_value")
        ddl_primary_key = (pp.CaselessKeyword("PRIMARY") + pp.CaselessKeyword("KEY")).set_results_name("is_primary_key")
        ddl_ignored_keywords = [
            "CONSTRAINT",
            "FOREIGN",
            "KEY",
            "FULLTEXT",
            "INDEX",
            "UNIQUE",
            "CHECK",
            "PERIOD",
        ]
        cls.ddl_constraint = (
            pp.Or(map(
                pp.CaselessKeyword,
                sorted(ddl_ignored_keywords + ["PRIMARY"], reverse=True)
            ))
            + cls.ddl_expression
        ).set_results_name("is_constraint")

        # Column parser
        cls.ddl_column = pp.Group(
            ddl_name.set_results_name("name")
            + cls.ddl_type.set_results_name("type")
            + pp.Suppress(pp.Optional(ddl_width))
            + pp.Suppress(pp.Optional(ddl_timezone))
            + pp.ZeroOrMore(
                ddl_unsigned
                | ddl_not_null
                | pp.Suppress(pp.CaselessKeyword("NULL"))
                | ddl_auto_value
                | ddl_default_value
                | ddl_generated_value
                | ddl_primary_key
                | pp.Suppress(pp.OneOrMore(pp.Or(map(pp.CaselessKeyword, sorted(ddl_ignored_keywords, reverse=True)))))
                | pp.Suppress(cls.ddl_expression)
            )
        )

        # CREATE TABLE parser
        ddl_ct_basic = (
            pp.Suppress(pp.CaselessKeyword("CREATE"))
            + pp.Suppress(pp.Optional(pp.CaselessKeyword("OR") + pp.CaselessKeyword("REPLACE")))
            + pp.Suppress(pp.CaselessKeyword("TABLE"))
            + pp.Suppress(pp.Optional(pp.CaselessKeyword("IF") + pp.CaselessKeyword("NOT") + pp.CaselessKeyword("EXISTS")))
            + ddl_name.set_results_name("table_name")
            + ddl_left
            + pp.Group(pp.DelimitedList(pp.Suppress(cls.ddl_constraint) | cls.ddl_column)).set_results_name("columns")
            + ddl_right
        )
        def add_create_sql(text, loc, tokens):
            create = tokens.create
            create.value["create_sql"] = text[create.locn_start:create.locn_end]
        cls.ddl_ct_with_sql = pp.Located(ddl_ct_basic).set_results_name("create").set_parse_action(add_create_sql)

        # Main DDL parser
        cls.ddl = pp.OneOrMore(pp.Group(pp.Suppress(pp.SkipTo(ddl_ct_basic, False)) + cls.ddl_ct_with_sql)).set_results_name("tables")
        ddl_comment = pp.one_of(["--", "#"]) + pp.rest_of_line
        cls.ddl.ignore(ddl_comment)
        cls.ddl.parse_with_tabs()

    @classmethod
    def parse_ddls(cls, ddl_paths):
        try:
            return [cls.ddl.parse_file(path) for path in ddl_paths]
        except pp.ParseException as e:
            print("ERROR: failed to parse " + path)
            print(e.explain(1))
            sys.exit(ExitCode.STRANGE_PARSING)


class SelfTest:
    """Runs a self-test of the parser"""

    @classmethod
    def run(cls):
        print("Running self-test")
        DdlParser.initialize()
        cls._test_boolean()
        cls._test_integer()
        cls._test_serial()
        cls._test_floating_point()
        cls._test_text()
        cls._test_blob()
        cls._test_date()
        cls._test_time()
        cls._test_unknown()
        cls._test_date_time()
        cls._test_column()
        cls._test_constraint()
        cls._test_math_expression()
        cls._test_rational()
        cls._test_table()
        cls._test_primary_key_auto_increment()

    @staticmethod
    def _test_boolean():
        for t in DdlParser.ddl_boolean_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "boolean"

    @staticmethod
    def _test_integer():
        for t in DdlParser.ddl_integer_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "integral"

    @staticmethod
    def _test_serial():
        for t in DdlParser.ddl_serial_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "integral"
            assert result.has_serial_value

    @staticmethod
    def _test_floating_point():
        for t in DdlParser.ddl_floating_point_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "floating_point"

    @staticmethod
    def _test_text():
        for t in DdlParser.ddl_text_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "text"

    @staticmethod
    def _test_blob():
        for t in DdlParser.ddl_blob_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "blob"

    @staticmethod
    def _test_date():
        for t in DdlParser.ddl_date_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "date"

    @staticmethod
    def _test_date_time():
        for t in DdlParser.ddl_date_time_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "timestamp"

    @staticmethod
    def _test_time():
        for t in DdlParser.ddl_time_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "time"

    @staticmethod
    def _test_unknown():
        for t in ["cheesecake", "blueberry"]:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "UNKNOWN"

    @staticmethod
    def _test_column():
        test_data = [
            {
                "text": "\"id\" int(8) unsigned NOT NULL DEFAULT nextval('dk_id_seq'::regclass)",
                "expected": {
                    "name": "id",
                    "type": "integral",
                    "is_unsigned": True,
                    "not_null": True,
                    "has_auto_value": False,
                    "has_default_value": True,
                    "has_generated_value": False,
                    "has_serial_value": False,
                    "is_primary_key": False
                }
            },
            {
                "text": "\"fld\" int AUTO_INCREMENT",
                "expected": {
                    "name": "fld",
                    "type": "integral",
                    "is_unsigned": False,
                    "not_null": False,
                    "has_auto_value": True,
                    "has_default_value": False,
                    "has_generated_value": False,
                    "has_serial_value": False,
                    "is_primary_key": False
                }
            },
            {
                "text": "\"fld2\" int NOT NULL GENERATED ALWAYS AS abc+1",
                "expected": {
                    "name": "fld2",
                    "type": "integral",
                    "is_unsigned": False,
                    "not_null": True,
                    "has_auto_value": False,
                    "has_default_value": False,
                    "has_generated_value": True,
                    "has_serial_value": False,
                    "is_primary_key": False
                }
            }
        ]
        for td in test_data:
            result = DdlParser.ddl_column.parse_string(td["text"], parse_all=True)[0]
            expected = td["expected"]
            assert result.name == expected["name"]
            assert result.type == expected["type"]
            assert bool(result.is_unsigned) == expected["is_unsigned"]
            assert bool(result.not_null) == expected["not_null"]
            assert bool(result.has_auto_value) == expected["has_auto_value"]
            assert bool(result.has_default_value) == expected["has_default_value"]
            assert bool(result.has_generated_value) == expected["has_generated_value"]
            assert bool(result.has_serial_value) == expected["has_serial_value"]
            assert bool(result.is_primary_key) == expected["is_primary_key"]

    @staticmethod
    def _test_constraint():
        for text in [
            "CONSTRAINT unique_person UNIQUE (first_name, last_name)",
            "UNIQUE (id)",
            "UNIQUE (first_name,last_name)"
        ]:
            result = DdlParser.ddl_constraint.parse_string(text, parse_all=True)
            assert result.is_constraint

    @staticmethod
    def _test_math_expression():
            text = "2 DIV 2"
            result = DdlParser.ddl_expression.parse_string(text, parse_all=True)
            assert len(result) == 3
            assert result[0] == "2"
            assert result[1] == "DIV"
            assert result[2] == "2"

    @staticmethod
    def _test_rational():
        for text in [
            "pos RATIONAL NOT NULL DEFAULT nextval('rational_seq')::integer",
        ]:
            result = DdlParser.ddl_column.parse_string(text, parse_all=True)
            column = result[0]
            assert column.name == "pos"
            assert column.type == "text"
            assert column.not_null

    @staticmethod
    def _test_table():
        text = """
    CREATE TABLE "public"."dk" (
    "id" int8 NOT NULL DEFAULT nextval('dk_id_seq'::regclass),
    "strange" NUMERIC(314, 15),
    "last_update" timestamp(6) DEFAULT now(),
    PRIMARY KEY (id)
    )
    """
        result = DdlParser.ddl_ct_with_sql.parse_string(text, parse_all=True)

    @staticmethod
    def _test_primary_key_auto_increment():
        for text in [
            "CREATE TABLE tab (col INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY)", # mysql
            "CREATE TABLE tab (col INTEGER NOT NULL PRIMARY KEY AUTO_INCREMENT)", # mysql
            "CREATE TABLE tab (col INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT)", # sqlite
        ]:
            result = DdlParser.ddl_ct_with_sql.parse_string(text, parse_all=True)
            assert len(result) == 1
            table = result.create.value
            assert table.table_name == "tab"
            assert len(table.columns) == 1
            column = table.columns[0]
            assert not column.is_constraint
            assert column.name == "col"
            assert column.type == "integral"
            assert column.not_null
            assert column.has_auto_value
            assert column.is_primary_key
            assert table.create_sql == text


class ModelWriter:
    """This class uses the parsed DDL definitions to generate and write the C++ database model file(s)"""

    @classmethod
    def write(cls, parsed_ddls, args):
        if args.path_to_header:
            cls._create_header(parsed_ddls, args)
        if args.path_to_header_directory:
            cls._create_split_headers(parsed_ddls, args)
        if args.path_to_module:
            cls._create_module(parsed_ddls, args)

    @classmethod
    def _create_header(cls, parsed_ddls, args):
        header = cls._begin_header(args.path_to_header, args)
        for pd in parsed_ddls:
            for table in pd.tables:
                cls._write_table(table, header, args)
        cls._end_header(header, args)

    @classmethod
    def _create_split_headers(cls, parsed_ddls, args):
        for pd in parsed_ddls:
            for table in pd.tables:
                sql_table_name = table.create.value.table_name
                header = cls._begin_header(os.path.join(args.path_to_header_directory, cls._to_class_name(sql_table_name, args) + ".h"), args)
                cls._write_table(table, header, args)
                cls._end_header(header, args)

    @staticmethod
    def _begin_header(path_to_header, args):
        header = open(path_to_header, "w")
        print("#pragma once", file=header)
        print("", file=header)
        print("// clang-format off", file=header)
        print("// generated by " + " ".join(sys.argv), file=header)
        print("", file=header)
        if args.use_import_std:
            print("import std;", file=header)
        else:
            print("#include <optional>", file=header)
        if args.use_import_sqlpp23:
            print("import sqlpp23.core;", file=header)
            print("", file=header)
            print("#include <sqlpp23/core/name/create_name_tag.h>", file=header)
        else:
            print("", file=header)
            print("#include <sqlpp23/core/basic/table.h>", file=header)
            print("#include <sqlpp23/core/basic/table_columns.h>", file=header)
            print("#include <sqlpp23/core/name/create_name_tag.h>", file=header)
            print("#include <sqlpp23/core/type_traits.h>", file=header)
        print("", file=header)
        print("namespace " + args.namespace + " {", file=header)
        return header

    @staticmethod
    def _end_header(header, args):
        print("} // namespace " + args.namespace, file=header)
        header.close()

    @classmethod
    def _create_module(cls, parsed_ddls, args):
        module = cls._begin_module(args.path_to_module, args)
        for pd in parsed_ddls:
            for table in pd.tables:
                cls._write_table(table, module, args)
        cls._end_module(module, args)

    @staticmethod
    def _begin_module(path_to_module, args):
        module = open(path_to_module, "w")
        print("module;", file=module)
        print("", file=module)
        print("// clang-format off", file=module)
        print("// generated by " + " ".join(sys.argv), file=module)
        print("", file=module)
        if args.use_import_std:
            print("import std;", file=module)
        else:
            print("#include <optional>", file=module)
        print("", file=module)
        print("#include <sqlpp23/core/name/create_name_tag.h>", file=module)
        print("", file=module)
        print("import sqlpp23.core;", file=module)
        print("", file=module)
        print("export module " + args.module_name + ";", file=module)
        print("", file=module)
        print("namespace " + args.namespace + " {", file=module)
        return module

    @staticmethod
    def _end_module(module, args):
        print("} // namespace " + args.namespace, file=module)
        module.close()

    @classmethod
    def _write_table(cls, table, header, args):
        export = "export " if args.path_to_module else ""
        data_type_error = False
        create = table.create.value
        sql_table_name = create.table_name
        table_class = cls._to_class_name(sql_table_name, args)
        table_member = cls._to_member_name(sql_table_name, args)
        table_spec = table_class + "_"
        table_template_parameters = ""
        table_required_insert_columns = ""
        if args.generate_table_creation_helper:
            creation_helper_func = "create" + ("" if args.naming_style == "camel-case" else "_") + table_class
            print("  " + export + "template<typename Db>", file=header)
            print("  void " + creation_helper_func + "(Db& db) {", file=header)
            print("    db(R\"+++(DROP TABLE IF EXISTS " + sql_table_name + ")+++\");", file=header)
            print("    db(R\"+++(" + create.create_sql + ")+++\");", file=header)
            print("  }", file=header)
            print("", file=header)
        print("  " + export + "struct " + table_spec + " {", file=header)
        for column in create.columns:
            if column.is_constraint:
                continue
            sql_column_name = column.name
            column_class = cls._to_class_name(sql_column_name, args)
            column_member = cls._to_member_name(sql_column_name, args)
            column_type = column.type
            if column_type == "UNKNOWN":
                print(
                    "Error: SQL data type of %s.%s is not supported."
                    % (sql_table_name, sql_column_name)
                )
                data_type_error = True
            if column_type == "integral" and column.is_unsigned:
                column_type = "unsigned_" + column_type
            if column_type == "timestamp" and not args.suppress_timestamp_warning:
                args.suppress_timestamp_warning = True
                print(
                    "Warning: date and time values are assumed to be without timezone."
                )
                print(
                    "Warning: If you are using types WITH timezones, your code has to deal with that."
                )
                print("You can disable this warning using --suppress-timestamp-warning")
            print("    struct " + column_class + " {", file=header)
            print("      SQLPP_CREATE_NAME_TAG_FOR_SQL_AND_CPP("
                + cls._escape_if_reserved(sql_column_name) + ", " + column_member + ");"
                , file=header)
            column_is_const = column.has_generated_value
            const_prefix = "const " if column_is_const else ""
            column_can_be_null = not column.not_null and not column.is_primary_key and not column.has_serial_value
            if column_can_be_null:
                print("      using data_type = " + const_prefix + "std::optional<::sqlpp::" + column_type + ">;", file=header)
            else:
                print("      using data_type = " + const_prefix + "::sqlpp::" + column_type + ";", file=header)
            column_has_default = column.has_default_value or \
                            column.has_serial_value or \
                            column.has_auto_value or \
                            column.has_generated_value or \
                            (args.assume_auto_id and sql_column_name == "id") or \
                            column_can_be_null
            if column_has_default:
                print("      using has_default = std::true_type;", file=header)
            else:
                print("      using has_default = std::false_type;", file=header)
            print("    };", file=header)
            if table_template_parameters:
                table_template_parameters += ","
            table_template_parameters += "\n               " + column_class
            if not column_has_default:
                if table_required_insert_columns:
                    table_required_insert_columns += ","
                table_required_insert_columns += "\n               sqlpp::column_t<sqlpp::table_t<" + table_spec + ">, " + column_class + ">";
        print("    SQLPP_CREATE_NAME_TAG_FOR_SQL_AND_CPP("
            + cls._escape_if_reserved(sql_table_name) + ", " + table_member + ");"
            , file=header)
        print("    template<typename T>", file=header)
        print("    using _table_columns = sqlpp::table_columns<T,"
            + table_template_parameters
            + ">;", file=header)
        print("    using _required_insert_columns = sqlpp::detail::type_set<"
            + table_required_insert_columns
            + ">;", file=header)
        print("  };", file=header)
        print(
            "  " + export + "using " + table_class + " = ::sqlpp::table_t<" + table_spec + ">;", file=header)
        print("", file=header)
        if data_type_error:
            print("Error: unsupported SQL data type(s).")
            print("Possible solutions:")
            print("A) Use the '--path-to-custom-types' command line argument to map the SQL data type to a known sqlpp23 data type (example: README)")
            print("B) Implement this data type in sqlpp23 (examples: sqlpp23/data_types) and in sqlpp23-ddl2cpp")
            print("C) Raise an issue on github")
            sys.exit(ExitCode.BAD_DATA_TYPE)  # return non-zero error code, we might need it for automation

    @classmethod
    def _to_class_name(cls, name, args):
        if args.naming_style == "camel-case":
            name = name.replace(".", "_")
            return re.sub(r"(^|\s|[_0-9])(\S)", cls._repl_camel_case_func, name)
        # otherwise return identity
        return name

    @classmethod
    def _to_member_name(cls, name, args):
        if args.naming_style == "camel-case":
            name = name.replace(".", "_")
            return re.sub(r"(\s|_|[0-9])(\S)", cls._repl_camel_case_func, name)
        # otherwise return identity
        return name

    @staticmethod
    def _repl_camel_case_func(m):
        if m.group(1) == "_":
            return m.group(2).upper()
        else:
            return m.group(1) + m.group(2).upper()

    @staticmethod
    def _escape_if_reserved(name):
        reserved_names = [
            "BEGIN",
            "END",
            "GROUP",
            "ORDER",
        ]
        if name.upper() in reserved_names:
            return "!{}".format(name)
        return name


def parse_commandline_args():
    arg_parser = argparse.ArgumentParser(prog="sqlpp23-ddl2cpp")
    required = arg_parser.add_argument_group("Required parameters for code generation")
    required.add_argument("--path-to-ddl", nargs="*", help="one or more path(s) to DDL input file(s)")
    required.add_argument("--namespace", help="namespace for generated table classes")

    paths = arg_parser.add_argument_group("Paths", "Choose one or more paths for code generation:")
    paths.add_argument("--path-to-module", help="path to generated module file (also requires --module-name)")
    paths.add_argument("--path-to-header", help="path to generated header file (one file for all tables)")
    paths.add_argument("--path-to-header-directory", help="path to directory for generated header files (one file per table)")
    paths.add_argument("--path-to-custom-types", help="path to csv file defining aliases of existing SQL data types")

    options = arg_parser.add_argument_group("Additional options")
    options.add_argument("--module-name", help="name of the generated module (to be used with --path-to-module)")
    options.add_argument("--suppress-timestamp-warning", action="store_true", help="suppress show warning about date / time data types")
    options.add_argument("--assume-auto-id", action="store_true", help="assume column 'id' to have an automatic value as if AUTO_INCREMENT was specified (e.g. implicit for SQLite ROWID (default: False)")
    options.add_argument("--naming-style", choices=["camel-case", "identity"], default="camel-case", help="naming style for generated tables and columns.\n\n\n\n 'camel-case' (default): interprets '_' as word separator and translates table names to UpperCamelCase and column names to lowerCamelCase, e.g. 'my_cool_table.important_column' will be represented as 'MyCoolTable{}.importantColumn' in generated code.\n 'identity' uses table and column names as is in generated code (default: 'camel-case')")
    options.add_argument("--generate-table-creation-helper", action="store_true", help="create a helper function for each table that drops and creates the table")
    options.add_argument("--use-import-sqlpp23", action="store_true", help="import sqlpp23 as module instead of including the header file (default: False)")
    options.add_argument("--use-import-std", action="store_true", help="import std as module instead of including the respective standard header files (default: False)")
    options.add_argument("--self-test", action="store_true", help="run parser self-test (this ignores all other arguments)")

    args = arg_parser.parse_args()

    if args.self_test:
        return args

    if not args.path_to_ddl or not len(args.path_to_ddl):
        print("Missing argument --path-to-ddl")
        arg_parser.print_help()
        sys.exit(ExitCode.BAD_ARGS)

    if not args.namespace:
        print("Missing argument --namespace")
        arg_parser.print_help()
        sys.exit(ExitCode.BAD_ARGS)

    if not args.path_to_module and not args.path_to_header and not args.path_to_header_directory:
        print("Missing argument(s): at least one path for code generation")
        arg_parser.print_help()
        sys.exit(ExitCode.BAD_ARGS)

    if args.path_to_module and not args.module_name:
        print("Missing argument --module-name")
        arg_parser.print_help()
        sys.exit(ExitCode.BAD_ARGS)

    return args


def get_custom_types(filename):
    if not filename:
        return None
    import csv
    with open(filename, newline="") as csv_file:
        reader = csv.DictReader(csv_file, fieldnames=["base_type"], restkey="custom_types", delimiter=",")
        types = {}
        for row in reader:
            var_values = [clean_val for value in row["custom_types"] if (clean_val := value.strip(" \"'").lower())]
            if var_values:
                types[row["base_type"]] = var_values
        return types


if __name__ == "__main__":
    args = parse_commandline_args()

    if args.self_test:
        SelfTest.run()
    else:
        custom_types = get_custom_types(args.path_to_custom_types)
        DdlParser.initialize(custom_types)
        parsed_ddls = DdlParser.parse_ddls(args.path_to_ddl)
        ModelWriter.write(parsed_ddls, args)
    sys.exit(ExitCode.SUCCESS)

#!/usr/bin/env python3

##
# Copyright (c) 2013, Roland Bock
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without modification,
# are permitted provided that the following conditions are met:
#
#  * Redistributions of source code must retain the above copyright notice,
#    this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright notice,
#    this list of conditions and the following disclaimer in the documentation
#    and/or other materials provided with the distribution.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
# IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,
# INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
# LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
# OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
# OF THE POSSIBILITY OF SUCH DAMAGE.
##

import argparse
import pyparsing as pp
import sys
import re
import os


class ExitCode:
    SUCCESS = 0
    BAD_ARGS = 1
    BAD_DATA_TYPE = 10
    STRANGE_PARSING = 20


# DdlParser, SelfTest and ModelWriter could be modules instead of classes. However, using
# modules implies that the program code would be split into several files and we want to avoid that
# because it would complicate program installation. That is why we use classes.


class DdlParser:
    """Rather crude SQL expression parser.
    This is not geared at correctly interpreting SQL, but at identifying (and ignoring) expressions for instance in DEFAULT expressions
    """

    # Names of data types that can be updated by the custom data types
    ddlBooleanTypes = [
        "bool",
        "boolean",
    ]
    ddlIntegerTypes = [
        "bigint",
        "int",
        "int2",  # PostgreSQL
        "int4",  # PostgreSQL
        "int8",  # PostgreSQL
        "integer",
        "mediumint",
        "smallint",
        "tinyint",
    ]
    ddlSerialTypes = [
        "bigserial",  # PostgreSQL
        "serial",  # PostgreSQL
        "serial2",  # PostgreSQL
        "serial4",  # PostgreSQL
        "serial8",  # PostgreSQL
        "smallserial",  # PostgreSQL
    ]
    ddlFloatingPointTypes = [
        "decimal",  # MYSQL
        "double",
        "float8",  # PostgreSQL
        "float",
        "float4",  # PostgreSQL
        "numeric",  # PostgreSQL
        "real",
    ]
    ddlTextTypes = [
        "char",
        "varchar",
        "character varying",  # PostgreSQL
        "text",
        "clob",
        "enum",  # MYSQL
        "set",
        "longtext",  # MYSQL
        "jsonb",  # PostgreSQL
        "json",  # PostgreSQL
        "tinytext",  # MYSQL
        "mediumtext",  # MYSQL
        "rational", # PostgreSQL pg_rationale extension
    ]
    ddlBlobTypes = [
        "bytea",
        "tinyblob",
        "blob",
        "mediumblob",
        "longblob",
        "binary",  # MYSQL
        "varbinary",  # MYSQL
    ]
    ddlDateTypes = [
        "date",
    ]
    ddlDateTimeTypes = [
        "datetime",
        "timestamp",
        "timestamp without time zone",  # PostgreSQL
        "timestamp with time zone",  # PostgreSQL
        "timestamptz",  # PostgreSQL
    ]
    ddlTimeTypes = [
        "time",
        "time without time zone",  # PostgreSQL
        "time with time zone",  # PostgreSQL
    ]

    # Parsers that are initialized later
    ddlExpression = None
    ddlType = None
    ddlColumn = None
    ddlConstraint = None
    ddlCtWithSql = None
    ddl = None

    @classmethod
    def initialize(cls, customTypes=None):
        """Initialize the DDL parser"""

        # Basic parsers
        ddlLeft = pp.Suppress("(")
        ddlRight = pp.Suppress(")")
        ddlNumber = pp.Word(pp.nums + "+-.", pp.nums + "+-.Ee")
        ddlString = (
            pp.QuotedString("'") | pp.QuotedString('"', escQuote='""') | pp.QuotedString("`")
        )
        # ddlString.setDebug(True) #uncomment to debug pyparsing
        ddlTerm = pp.Word(pp.alphas + "_", pp.alphanums + "_.$")
        ddlName = pp.Or([ddlTerm, ddlString, pp.Combine(ddlString + "." + ddlString), pp.Combine(ddlTerm + ddlString)])
        ddlOperator = pp.Or(
            map(pp.CaselessLiteral, ["+", "-", "*", "/", "<", "<=", ">", ">=", "=", "%"]),
            pp.CaselessKeyword("DIV")
        )
        ddlBracedExpression = pp.Forward()
        ddlFunctionCall = pp.Forward()
        ddlCastEnd = "::" + ddlTerm
        ddlCast = ddlString + ddlCastEnd
        ddlBracedArguments = pp.Forward()
        cls.ddlExpression = pp.OneOrMore(
            ddlBracedExpression
            | ddlFunctionCall
            | ddlCastEnd
            | ddlCast
            | ddlOperator
            | ddlString
            | ddlTerm
            | ddlNumber
            | ddlBracedArguments
        )
        ddlBracedArguments << ddlLeft + pp.delimitedList(cls.ddlExpression) + ddlRight
        ddlBracedExpression << ddlLeft + cls.ddlExpression + ddlRight
        ddlArguments = pp.Suppress(pp.delimitedList(cls.ddlExpression))
        ddlFunctionCall << ddlName + ddlLeft + pp.Optional(ddlArguments) + ddlRight

        # Data type parsers
        def get_type_parser(key, data_type):
            type_names = getattr(cls, f"ddl{key}Types")
            if customTypes and (key in customTypes):
                type_names.extend(customTypes[key])
            return pp.Or(
                map(pp.CaselessKeyword, sorted(type_names, reverse=True))
            ).setParseAction(pp.replaceWith(data_type))

        ddlBoolean = get_type_parser("Boolean", "boolean")
        ddlInteger = get_type_parser("Integer", "integral")
        ddlSerial = get_type_parser("Serial", "integral").setResultsName("hasSerialValue")
        ddlFloatingPoint = get_type_parser("FloatingPoint", "floating_point")
        ddlText = get_type_parser("Text", "text")
        ddlBlob = get_type_parser("Blob", "blob")
        ddlDate = get_type_parser("Date", "date").setResultsName("warnTimezone")
        ddlDateTime = get_type_parser("DateTime", "timestamp")
        ddlTime = get_type_parser("Time", "time")
        ddlUnknown = pp.Word(pp.alphanums).setParseAction(pp.replaceWith("UNKNOWN"))
        cls.ddlType = (
            ddlBoolean
            | ddlInteger
            | ddlSerial
            | ddlFloatingPoint
            | ddlText
            | ddlBlob
            | ddlDateTime
            | ddlDate
            | ddlTime
            | ddlUnknown
        )

        # Constraints parser
        ddlUnsigned = pp.CaselessKeyword("UNSIGNED").setResultsName("isUnsigned")
        ddlDigits = "," + pp.Word(pp.nums)
        ddlWidth = ddlLeft + pp.Word(pp.nums) + pp.Optional(ddlDigits) + ddlRight
        ddlTimezone = (
            (pp.CaselessKeyword("with") | pp.CaselessKeyword("without"))
            + pp.CaselessKeyword("time")
            + pp.CaselessKeyword("zone")
        )
        ddlNotNull = (pp.CaselessKeyword("NOT") + pp.CaselessKeyword("NULL")).setResultsName("notNull")
        ddlDefaultValue = pp.CaselessKeyword("DEFAULT").setResultsName("hasDefaultValue")
        ddlGeneratedValue = pp.CaselessKeyword("GENERATED").setResultsName("hasGeneratedValue")
        ddlAutoKeywords = [
            "AUTO_INCREMENT",
            "AUTOINCREMENT"
        ]
        ddlAutoValue = pp.Or(map(pp.CaselessKeyword, sorted(ddlAutoKeywords, reverse=True))).setResultsName("hasAutoValue")
        ddlPrimaryKey = (pp.CaselessKeyword("PRIMARY") + pp.CaselessKeyword("KEY")).setResultsName("isPrimaryKey")
        ddlIgnoredKeywords = [
            "CONSTRAINT",
            "FOREIGN",
            "KEY",
            "FULLTEXT",
            "INDEX",
            "UNIQUE",
            "CHECK",
            "PERIOD",
        ]
        cls.ddlConstraint = (
            pp.Or(map(
                pp.CaselessKeyword,
                sorted(ddlIgnoredKeywords + ["PRIMARY"], reverse=True)
            ))
            + cls.ddlExpression
        ).setResultsName("isConstraint")

        # Column parser
        cls.ddlColumn = pp.Group(
            ddlName.setResultsName("name")
            + cls.ddlType.setResultsName("type")
            + pp.Suppress(pp.Optional(ddlWidth))
            + pp.Suppress(pp.Optional(ddlTimezone))
            + pp.ZeroOrMore(
                ddlUnsigned
                | ddlNotNull
                | pp.Suppress(pp.CaselessKeyword("NULL"))
                | ddlAutoValue
                | ddlDefaultValue
                | ddlGeneratedValue
                | ddlPrimaryKey
                | pp.Suppress(pp.OneOrMore(pp.Or(map(pp.CaselessKeyword, sorted(ddlIgnoredKeywords, reverse=True)))))
                | pp.Suppress(cls.ddlExpression)
            )
        )

        # CREATE TABLE parser
        ddlCtBasic = (
            pp.Suppress(pp.CaselessKeyword("CREATE"))
            + pp.Suppress(pp.Optional(pp.CaselessKeyword("OR") + pp.CaselessKeyword("REPLACE")))
            + pp.Suppress(pp.CaselessKeyword("TABLE"))
            + pp.Suppress(pp.Optional(pp.CaselessKeyword("IF") + pp.CaselessKeyword("NOT") + pp.CaselessKeyword("EXISTS")))
            + ddlName.setResultsName("tableName")
            + ddlLeft
            + pp.Group(pp.delimitedList(pp.Suppress(cls.ddlConstraint) | cls.ddlColumn)).setResultsName("columns")
            + ddlRight
        )
        def addCreateSql(text, loc, tokens):
            create = tokens.create
            create.value["createSql"] = text[create.locn_start:create.locn_end]
        cls.ddlCtWithSql = pp.Located(ddlCtBasic).setResultsName("create").setParseAction(addCreateSql)

        # Main DDL parser
        cls.ddl = pp.OneOrMore(pp.Group(pp.Suppress(pp.SkipTo(ddlCtBasic, False)) + cls.ddlCtWithSql)).setResultsName("tables")
        ddlComment = pp.oneOf(["--", "#"]) + pp.restOfLine
        cls.ddl.ignore(ddlComment)
        cls.ddl.parseWithTabs()

    @classmethod
    def parse_ddls(cls, ddl_paths):
        try:
            return [cls.ddl.parseFile(path) for path in ddl_paths]
        except pp.ParseException as e:
            print("ERROR: failed to parse " + path)
            print(e.explain(1))
            sys.exit(ExitCode.STRANGE_PARSING)


class SelfTest:
    """Runs a self-test of the parser"""

    @classmethod
    def run(cls):
        print("Running self-test")
        DdlParser.initialize()
        cls.testBoolean()
        cls.testInteger()
        cls.testSerial()
        cls.testFloatingPoint()
        cls.testText()
        cls.testBlob()
        cls.testDate()
        cls.testTime()
        cls.testUnknown()
        cls.testDateTime()
        cls.testColumn()
        cls.testConstraint()
        cls.testMathExpression()
        cls.testRational()
        cls.testTable()
        cls.testPrimaryKeyAutoIncrement()

    @staticmethod
    def testBoolean():
        for t in DdlParser.ddlBooleanTypes:
            result = DdlParser.ddlType.parseString(t, parseAll=True)
            assert result[0] == "boolean"

    @staticmethod
    def testInteger():
        for t in DdlParser.ddlIntegerTypes:
            result = DdlParser.ddlType.parseString(t, parseAll=True)
            assert result[0] == "integral"

    @staticmethod
    def testSerial():
        for t in DdlParser.ddlSerialTypes:
            result = DdlParser.ddlType.parseString(t, parseAll=True)
            assert result[0] == "integral"
            assert result.hasSerialValue

    @staticmethod
    def testFloatingPoint():
        for t in DdlParser.ddlFloatingPointTypes:
            result = DdlParser.ddlType.parseString(t, parseAll=True)
            assert result[0] == "floating_point"

    @staticmethod
    def testText():
        for t in DdlParser.ddlTextTypes:
            result = DdlParser.ddlType.parseString(t, parseAll=True)
            assert result[0] == "text"

    @staticmethod
    def testBlob():
        for t in DdlParser.ddlBlobTypes:
            result = DdlParser.ddlType.parseString(t, parseAll=True)
            assert result[0] == "blob"

    @staticmethod
    def testDate():
        for t in DdlParser.ddlDateTypes:
            result = DdlParser.ddlType.parseString(t, parseAll=True)
            assert result[0] == "date"

    @staticmethod
    def testDateTime():
        for t in DdlParser.ddlDateTimeTypes:
            result = DdlParser.ddlType.parseString(t, parseAll=True)
            assert result[0] == "timestamp"

    @staticmethod
    def testTime():
        for t in DdlParser.ddlTimeTypes:
            result = DdlParser.ddlType.parseString(t, parseAll=True)
            assert result[0] == "time"

    @staticmethod
    def testUnknown():
        for t in ["cheesecake", "blueberry"]:
            result = DdlParser.ddlType.parseString(t, parseAll=True)
            assert result[0] == "UNKNOWN"

    @staticmethod
    def testColumn():
        testData = [
            {
                "text": "\"id\" int(8) unsigned NOT NULL DEFAULT nextval('dk_id_seq'::regclass)",
                "expected": {
                    "name": "id",
                    "type": "integral",
                    "isUnsigned": True,
                    "notNull": True,
                    "hasAutoValue": False,
                    "hasDefaultValue": True,
                    "hasGeneratedValue": False,
                    "hasSerialValue": False,
                    "isPrimaryKey": False
                }
            },
            {
                "text": "\"fld\" int AUTO_INCREMENT",
                "expected": {
                    "name": "fld",
                    "type": "integral",
                    "isUnsigned": False,
                    "notNull": False,
                    "hasAutoValue": True,
                    "hasDefaultValue": False,
                    "hasGeneratedValue": False,
                    "hasSerialValue": False,
                    "isPrimaryKey": False
                }
            },
            {
                "text": "\"fld2\" int NOT NULL GENERATED ALWAYS AS abc+1",
                "expected": {
                    "name": "fld2",
                    "type": "integral",
                    "isUnsigned": False,
                    "notNull": True,
                    "hasAutoValue": False,
                    "hasDefaultValue": False,
                    "hasGeneratedValue": True,
                    "hasSerialValue": False,
                    "isPrimaryKey": False
                }
            }
        ]
        for td in testData:
            result = DdlParser.ddlColumn.parseString(td["text"], parseAll=True)[0]
            expected = td["expected"]
            assert result.name == expected["name"]
            assert result.type == expected["type"]
            assert bool(result.isUnsigned) == expected["isUnsigned"]
            assert bool(result.notNull) == expected["notNull"]
            assert bool(result.hasAutoValue) == expected["hasAutoValue"]
            assert bool(result.hasDefaultValue) == expected["hasDefaultValue"]
            assert bool(result.hasGeneratedValue) == expected["hasGeneratedValue"]
            assert bool(result.hasSerialValue) == expected["hasSerialValue"]
            assert bool(result.isPrimaryKey) == expected["isPrimaryKey"]

    @staticmethod
    def testConstraint():
        for text in [
            "CONSTRAINT unique_person UNIQUE (first_name, last_name)",
            "UNIQUE (id)",
            "UNIQUE (first_name,last_name)"
        ]:
            result = DdlParser.ddlConstraint.parseString(text, parseAll=True)
            assert result.isConstraint

    @staticmethod
    def testMathExpression():
            text = "2 DIV 2"
            result = DdlParser.ddlExpression.parseString(text, parseAll=True)
            assert len(result) == 3
            assert result[0] == "2"
            assert result[1] == "DIV"
            assert result[2] == "2"

    @staticmethod
    def testRational():
        for text in [
            "pos RATIONAL NOT NULL DEFAULT nextval('rational_seq')::integer",
        ]:
            result = DdlParser.ddlColumn.parseString(text, parseAll=True)
            column = result[0]
            assert column.name == "pos"
            assert column.type == "text"
            assert column.notNull

    @staticmethod
    def testTable():
        text = """
    CREATE TABLE "public"."dk" (
    "id" int8 NOT NULL DEFAULT nextval('dk_id_seq'::regclass),
    "strange" NUMERIC(314, 15),
    "last_update" timestamp(6) DEFAULT now(),
    PRIMARY KEY (id)
    )
    """
        result = DdlParser.ddlCtWithSql.parseString(text, parseAll=True)

    @staticmethod
    def testPrimaryKeyAutoIncrement():
        for text in [
            "CREATE TABLE tab (col INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY)", # mysql
            "CREATE TABLE tab (col INTEGER NOT NULL PRIMARY KEY AUTO_INCREMENT)", # mysql
            "CREATE TABLE tab (col INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT)", # sqlite
        ]:
            result = DdlParser.ddlCtWithSql.parseString(text, parseAll=True)
            assert len(result) == 1
            table = result.create.value
            assert table.tableName == "tab"
            assert len(table.columns) == 1
            column = table.columns[0]
            assert not column.isConstraint
            assert column.name == "col"
            assert column.type == "integral"
            assert column.notNull
            assert column.hasAutoValue
            assert column.isPrimaryKey
            assert table.createSql == text


class ModelWriter:
    """This class uses the parsed DDL definitions to generate and write the C++ database model file(s)"""

    @classmethod
    def write(cls, parsedDdls, args):
        if args.path_to_header:
            cls.createHeader(parsedDdls, args)
        if args.path_to_header_directory:
            cls.createSplitHeaders(parsedDdls, args)
        if args.path_to_module:
            cls.createModule(parsedDdls, args)

    @classmethod
    def createHeader(cls, parsedDdls, args):
        header = cls.beginHeader(args.path_to_header, args)
        for pd in parsedDdls:
            for table in pd.tables:
                cls.writeTable(table, header, args)
        cls.endHeader(header, args)

    @classmethod
    def createSplitHeaders(cls, parsedDdls, args):
        for pd in parsedDdls:
            for table in pd.tables:
                sqlTableName = table.create.value.tableName
                header = cls.beginHeader(os.path.join(args.path_to_header_directory, cls.toClassName(sqlTableName, args) + ".h"), args)
                cls.writeTable(table, header, args)
                cls.endHeader(header, args)

    @staticmethod
    def beginHeader(pathToHeader, args):
        header = open(pathToHeader, "w")
        print("#pragma once", file=header)
        print("", file=header)
        print("// clang-format off", file=header)
        print("// generated by " + " ".join(sys.argv), file=header)
        print("", file=header)
        if args.use_import_std:
            print("import std;", file=header)
        else:
            print("#include <optional>", file=header)
        if args.use_import_sqlpp23:
            print("import sqlpp23.core;", file=header)
            print("", file=header)
            print("#include <sqlpp23/core/name/create_name_tag.h>", file=header)
        else:
            print("", file=header)
            print("#include <sqlpp23/core/basic/table.h>", file=header)
            print("#include <sqlpp23/core/basic/table_columns.h>", file=header)
            print("#include <sqlpp23/core/name/create_name_tag.h>", file=header)
            print("#include <sqlpp23/core/type_traits.h>", file=header)
        print("", file=header)
        print("namespace " + args.namespace + " {", file=header)
        return header

    @staticmethod
    def endHeader(header, args):
        print("} // namespace " + args.namespace, file=header)
        header.close()

    @classmethod
    def createModule(cls, parsedDdls, args):
        module = cls.beginModule(args.path_to_module, args)
        for pd in parsedDdls:
            for table in pd.tables:
                cls.writeTable(table, module, args)
        cls.endModule(module, args)

    @staticmethod
    def beginModule(pathToModule, args):
        module = open(pathToModule, "w")
        print("module;", file=module)
        print("", file=module)
        print("// clang-format off", file=module)
        print("// generated by " + " ".join(sys.argv), file=module)
        print("", file=module)
        if args.use_import_std:
            print("import std;", file=module)
        else:
            print("#include <optional>", file=module)
        print("", file=module)
        print("#include <sqlpp23/core/name/create_name_tag.h>", file=module)
        print("", file=module)
        print("import sqlpp23.core;", file=module)
        print("", file=module)
        print("export module " + args.module_name + ";", file=module)
        print("", file=module)
        print("namespace " + args.namespace + " {", file=module)
        return module

    @staticmethod
    def endModule(module, args):
        print("} // namespace " + args.namespace, file=module)
        module.close()

    @classmethod
    def writeTable(cls, table, header, args):
        export = "export " if args.path_to_module else ""
        DataTypeError = False
        create = table.create.value
        sqlTableName = create.tableName
        tableClass = cls.toClassName(sqlTableName, args)
        tableMember = cls.toMemberName(sqlTableName, args)
        tableSpec = tableClass + "_"
        tableTemplateParameters = ""
        tableRequiredInsertColumns = ""
        if args.generate_table_creation_helper:
            creationHelperFunc = "create" + ("" if args.naming_style == "camel-case" else "_") + tableClass
            print("  " + export + "template<typename Db>", file=header)
            print("  void " + creationHelperFunc + "(Db& db) {", file=header)
            print("    db(R\"+++(DROP TABLE IF EXISTS " + sqlTableName + ")+++\");", file=header)
            print("    db(R\"+++(" + create.createSql + ")+++\");", file=header)
            print("  }", file=header)
            print("", file=header)
        print("  " + export + "struct " + tableSpec + " {", file=header)
        for column in create.columns:
            if column.isConstraint:
                continue
            sqlColumnName = column.name
            columnClass = cls.toClassName(sqlColumnName, args)
            columnMember = cls.toMemberName(sqlColumnName, args)
            columnType = column.type
            if columnType == "UNKNOWN":
                print(
                    "Error: datatype of %s.%s is not supported."
                    % (sqlTableName, sqlColumnName)
                )
                DataTypeError = True
            if columnType == "integral" and column.isUnsigned:
                columnType = "unsigned_" + columnType
            if columnType == "timestamp" and not args.suppress_timestamp_warning:
                args.suppress_timestamp_warning = True
                print(
                    "Warning: date and time values are assumed to be without timezone."
                )
                print(
                    "Warning: If you are using types WITH timezones, your code has to deal with that."
                )
                print("You can disable this warning using --suppress-timestamp-warning")
            print("    struct " + columnClass + " {", file=header)
            print("      SQLPP_CREATE_NAME_TAG_FOR_SQL_AND_CPP("
                + cls.escape_if_reserved(sqlColumnName) + ", " + columnMember + ");"
                , file=header)
            columnIsConst = column.hasGeneratedValue
            constPrefix = "const " if columnIsConst else ""
            columnCanBeNull = not column.notNull and not column.isPrimaryKey and not column.hasSerialValue
            if columnCanBeNull:
                print("      using data_type = " + constPrefix + "std::optional<::sqlpp::" + columnType + ">;", file=header)
            else:
                print("      using data_type = " + constPrefix + "::sqlpp::" + columnType + ";", file=header)
            columnHasDefault = column.hasDefaultValue or \
                            column.hasSerialValue or \
                            column.hasAutoValue or \
                            column.hasGeneratedValue or \
                            (args.assume_auto_id and sqlColumnName == "id") or \
                            columnCanBeNull
            if columnHasDefault:
                print("      using has_default = std::true_type;", file=header)
            else:
                print("      using has_default = std::false_type;", file=header)
            print("    };", file=header)
            if tableTemplateParameters:
                tableTemplateParameters += ","
            tableTemplateParameters += "\n               " + columnClass
            if not columnHasDefault:
                if tableRequiredInsertColumns:
                    tableRequiredInsertColumns += ","
                tableRequiredInsertColumns += "\n               sqlpp::column_t<sqlpp::table_t<" + tableSpec + ">, " + columnClass + ">";
        print("    SQLPP_CREATE_NAME_TAG_FOR_SQL_AND_CPP("
            + cls.escape_if_reserved(sqlTableName) + ", " + tableMember + ");"
            , file=header)
        print("    template<typename T>", file=header)
        print("    using _table_columns = sqlpp::table_columns<T,"
            + tableTemplateParameters
            + ">;", file=header)
        print("    using _required_insert_columns = sqlpp::detail::type_set<"
            + tableRequiredInsertColumns
            + ">;", file=header)
        print("  };", file=header)
        print(
            "  " + export + "using " + tableClass + " = ::sqlpp::table_t<" + tableSpec + ">;", file=header)
        print("", file=header)
        if DataTypeError:
            print("Error: unsupported SQL data type(s).")
            print("Possible solutions:")
            print("A) Use the '--path-to-datatype-file' command line argument to map the SQL data type to a known sqlpp23 data type (example: README)")
            print("B) Implement this data type in sqlpp23 (examples: sqlpp23/data_types) and in sqlpp23-ddl2cpp")
            print("C) Raise an issue on github")
            sys.exit(ExitCode.BAD_DATA_TYPE)  # return non-zero error code, we might need it for automation

    @classmethod
    def toClassName(cls, name, args):
        if args.naming_style == "camel-case":
            name = name.replace(".", "_")
            return re.sub(r"(^|\s|[_0-9])(\S)", cls.repl_camel_case_func, name)
        # otherwise return identity
        return name

    @classmethod
    def toMemberName(cls, name, args):
        if args.naming_style == "camel-case":
            name = name.replace(".", "_")
            return re.sub(r"(\s|_|[0-9])(\S)", cls.repl_camel_case_func, name)
        # otherwise return identity
        return name

    @staticmethod
    def repl_camel_case_func(m):
        if m.group(1) == "_":
            return m.group(2).upper()
        else:
            return m.group(1) + m.group(2).upper()

    @staticmethod
    def escape_if_reserved(name):
        reserved_names = [
            "BEGIN",
            "END",
            "GROUP",
            "ORDER",
        ]
        if name.upper() in reserved_names:
            return "!{}".format(name)
        return name


def parseCommandlineArgs():
    argParser = argparse.ArgumentParser(prog="sqlpp23-ddl2cpp")
    required = argParser.add_argument_group("Required parameters for code generation")
    required.add_argument("--path-to-ddl", nargs="*", help="one or more path(s) to DDL input file(s)")
    required.add_argument("--namespace", help="namespace for generated table classes")

    paths = argParser.add_argument_group("Paths", "Choose one or more paths for code generation:")
    paths.add_argument("--path-to-module", help="path to generated module file (also requires --module-name)")
    paths.add_argument("--path-to-header", help="path to generated header file (one file for all tables)")
    paths.add_argument("--path-to-header-directory", help="path to directory for generated header files (one file per table)")
    paths.add_argument("--path-to-datatype-file", help="path to csv file containing additional sql2cpp file type mappings")

    options = argParser.add_argument_group("Additional options")
    options.add_argument("--module-name", help="name of the generated module (to be used with --path-to-module)")
    options.add_argument("--suppress-timestamp-warning", action="store_true", help="suppress show warning about date / time data types")
    options.add_argument("--assume-auto-id", action="store_true", help="assume column 'id' to have an automatic value as if AUTO_INCREMENT was specified (e.g. implicit for SQLite ROWID (default: False)")
    options.add_argument("--naming-style", choices=["camel-case", "identity"], default="camel-case", help="naming style for generated tables and columns.\n\n\n\n 'camel-case' (default): interprets '_' as word separator and translates table names to UpperCamelCase and column names to lowerCamelCase, e.g. 'my_cool_table.important_column' will be represented as 'MyCoolTable{}.importantColumn' in generated code.\n 'identity' uses table and column names as is in generated code (default: 'camel-case')")
    options.add_argument("--generate-table-creation-helper", action="store_true", help="create a helper function for each table that drops and creates the table")
    options.add_argument("--use-import-sqlpp23", action="store_true", help="import sqlpp23 as module instead of including the header file (default: False)")
    options.add_argument("--use-import-std", action="store_true", help="import std as module instead of including the respective standard header files (default: False)")
    options.add_argument("--self-test", action="store_true", help="run parser self-test (this ignores all other arguments)")

    args = argParser.parse_args()

    if args.self_test:
        return args

    if not args.path_to_ddl or not len(args.path_to_ddl):
        print("Missing argument --path-to-ddl")
        argParser.print_help()
        sys.exit(ExitCode.BAD_ARGS)

    if not args.namespace:
        print("Missing argument --namespace")
        argParser.print_help()
        sys.exit(ExitCode.BAD_ARGS)

    if not args.path_to_module and not args.path_to_header and not args.path_to_header_directory:
        print("Missing argument(s): at least one path for code generation")
        argParser.print_help()
        sys.exit(ExitCode.BAD_ARGS)

    if args.path_to_module and not args.module_name:
        print("Missing argument --module-name")
        argParser.print_help()
        sys.exit(ExitCode.BAD_ARGS)

    return args


def get_extended_types(filename):
    if not filename:
        return None
    import csv
    with open(filename, newline="") as csvfile:
        reader = csv.DictReader(csvfile, fieldnames=["baseType"], restkey="customTypes", delimiter=",")
        types = {}
        for row in reader:
            var_values = [clean_val for value in row["customTypes"] if (clean_val := value.strip(" \"'").lower())]
            if var_values:
                types[row["baseType"]] = var_values
        return types


if __name__ == "__main__":
    args = parseCommandlineArgs()

    if args.self_test:
        SelfTest.run()
    else:
        customTypes = get_extended_types(args.path_to_datatype_file)
        DdlParser.initialize(customTypes)
        parsedDdls = DdlParser.parse_ddls(args.path_to_ddl)
        ModelWriter.write(parsedDdls, args)
    sys.exit(ExitCode.SUCCESS)

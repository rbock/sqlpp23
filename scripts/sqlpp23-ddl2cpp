#!/usr/bin/env python3

##
# Copyright (c) 2013, Roland Bock
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without modification,
# are permitted provided that the following conditions are met:
#
#  * Redistributions of source code must retain the above copyright notice,
#    this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright notice,
#    this list of conditions and the following disclaimer in the documentation
#    and/or other materials provided with the distribution.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
# IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,
# INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
# LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
# OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
# OF THE POSSIBILITY OF SUCH DAMAGE.
##

import argparse
import pyparsing as pp
import sys
import re
import os


class ExitCode:
    SUCCESS = 0
    BAD_ARGS = 1
    BAD_DATA_TYPE = 10
    STRANGE_PARSING = 20


# Rather crude SQL expression parser.
# This is not geared at correctly interpreting SQL, but at identifying (and ignoring) expressions for instance in DEFAULT expressions
ddlLeft, ddlRight = map(pp.Suppress, "()")
ddlNumber = pp.Word(pp.nums + "+-.", pp.nums + "+-.Ee")
ddlString = (
    pp.QuotedString("'") | pp.QuotedString('"', escQuote='""') | pp.QuotedString("`")
)
ddlTerm = pp.Word(pp.alphas + "_", pp.alphanums + "_.$")
ddlName = pp.Or([ddlTerm, ddlString, pp.Combine(ddlString + "." + ddlString), pp.Combine(ddlTerm + ddlString)])
ddlOperator = pp.Or(
    map(pp.CaselessLiteral, ["+", "-", "*", "/", "<", "<=", ">", ">=", "=", "%"]),
    pp.CaselessKeyword("DIV")
)

ddlBracedExpression = pp.Forward()
ddlFunctionCall = pp.Forward()
ddlCastEnd = "::" + ddlTerm
ddlCast = ddlString + ddlCastEnd
ddlBracedArguments = pp.Forward()
ddlExpression = pp.OneOrMore(
    ddlBracedExpression
    | ddlFunctionCall
    | ddlCastEnd
    | ddlCast
    | ddlOperator
    | ddlString
    | ddlTerm
    | ddlNumber
    | ddlBracedArguments
)

ddlBracedArguments << ddlLeft + pp.delimitedList(ddlExpression) + ddlRight
ddlBracedExpression << ddlLeft + ddlExpression + ddlRight

ddlArguments = pp.Suppress(pp.delimitedList(ddlExpression))
ddlFunctionCall << ddlName + ddlLeft + pp.Optional(ddlArguments) + ddlRight

# Data types
ddlBooleanTypes = [
    "bool",
    "boolean",
]

ddlIntegerTypes = [
    "bigint",
    "int",
    "int2",  # PostgreSQL
    "int4",  # PostgreSQL
    "int8",  # PostgreSQL
    "integer",
    "mediumint",
    "smallint",
    "tinyint",
]

ddlSerialTypes = [
    "bigserial",  # PostgreSQL
    "serial",  # PostgreSQL
    "serial2",  # PostgreSQL
    "serial4",  # PostgreSQL
    "serial8",  # PostgreSQL
    "smallserial",  # PostgreSQL
]

ddlFloatingPointTypes = [
    "decimal",  # MYSQL
    "double",
    "float8",  # PostgreSQL
    "float",
    "float4",  # PostgreSQL
    "numeric",  # PostgreSQL
    "real",
]

ddlTextTypes = [
    "char",
    "varchar",
    "character varying",  # PostgreSQL
    "text",
    "clob",
    "enum",  # MYSQL
    "set",
    "longtext",  # MYSQL
    "jsonb",  # PostgreSQL
    "json",  # PostgreSQL
    "tinytext",  # MYSQL
    "mediumtext",  # MYSQL
    "rational", # PostgreSQL pg_rationale extension
]

ddlBlobTypes = [
    "bytea",
    "tinyblob",
    "blob",
    "mediumblob",
    "longblob",
    "binary",  # MYSQL
    "varbinary",  # MYSQL
]

ddlDateTypes = [
    "date",
]

ddlDateTimeTypes = [
    "datetime",
    "timestamp",
    "timestamp without time zone",  # PostgreSQL
    "timestamp with time zone",  # PostgreSQL
    "timestamptz",  # PostgreSQL
]

ddlTimeTypes = [
    "time",
    "time without time zone",  # PostgreSQL
    "time with time zone",  # PostgreSQL
]


# Init the DDL parser
def createDdlParser():
    global ddl
    global ddlType
    global ddlColumn
    global ddlConstraint
    global ddlCreateTable
    global parsedContent
    # Column and constraint parsers

    ddlBoolean = pp.Or(
        map(pp.CaselessKeyword, sorted(ddlBooleanTypes, reverse=True))
    ).setParseAction(pp.replaceWith("boolean"))

    ddlInteger = pp.Or(
        map(pp.CaselessKeyword, sorted(ddlIntegerTypes, reverse=True))
    ).setParseAction(pp.replaceWith("integral"))

    ddlSerial = (
        pp.Or(map(pp.CaselessKeyword, sorted(ddlSerialTypes, reverse=True)))
        .setParseAction(pp.replaceWith("integral"))
        .setResultsName("hasSerialValue")
    )

    ddlFloatingPoint = pp.Or(
        map(pp.CaselessKeyword, sorted(ddlFloatingPointTypes, reverse=True))
    ).setParseAction(pp.replaceWith("floating_point"))

    ddlText = pp.Or(
        map(pp.CaselessKeyword, sorted(ddlTextTypes, reverse=True))
    ).setParseAction(pp.replaceWith("text"))


    ddlBlob = pp.Or(
        map(pp.CaselessKeyword, sorted(ddlBlobTypes, reverse=True))
    ).setParseAction(pp.replaceWith("blob"))

    ddlDate = (
        pp.Or(map(pp.CaselessKeyword, sorted(ddlDateTypes, reverse=True)))
        .setParseAction(pp.replaceWith("date"))
        .setResultsName("warnTimezone")
    )

    ddlDateTime = pp.Or(
        map(pp.CaselessKeyword, sorted(ddlDateTimeTypes, reverse=True))
    ).setParseAction(pp.replaceWith("timestamp"))

    ddlTime = pp.Or(
        map(pp.CaselessKeyword, sorted(ddlTimeTypes, reverse=True))
    ).setParseAction(pp.replaceWith("time"))

    ddlUnknown = pp.Word(pp.alphanums).setParseAction(pp.replaceWith("UNKNOWN"))

    ddlType = (
        ddlBoolean
        | ddlInteger
        | ddlSerial
        | ddlFloatingPoint
        | ddlText
        | ddlBlob
        | ddlDateTime
        | ddlDate
        | ddlTime
        | ddlUnknown
    )

    ddlUnsigned = pp.CaselessKeyword("UNSIGNED").setResultsName("isUnsigned")
    ddlDigits = "," + pp.Word(pp.nums)
    ddlWidth = ddlLeft + pp.Word(pp.nums) + pp.Optional(ddlDigits) + ddlRight
    ddlTimezone = (
        (pp.CaselessKeyword("with") | pp.CaselessKeyword("without"))
        + pp.CaselessKeyword("time")
        + pp.CaselessKeyword("zone")
    )

    ddlNotNull = (pp.CaselessKeyword("NOT") + pp.CaselessKeyword("NULL")).setResultsName("notNull")
    ddlDefaultValue = pp.CaselessKeyword("DEFAULT").setResultsName("hasDefaultValue")

    ddlGeneratedValue = pp.CaselessKeyword("GENERATED").setResultsName("hasGeneratedValue")

    ddlAutoKeywords = [
        "AUTO_INCREMENT",
        "AUTOINCREMENT"
    ]
    ddlAutoValue = pp.Or(map(pp.CaselessKeyword, sorted(ddlAutoKeywords, reverse=True))).setResultsName("hasAutoValue")

    ddlPrimaryKey = (pp.CaselessKeyword("PRIMARY") + pp.CaselessKeyword("KEY")).setResultsName("isPrimaryKey")

    ddlIgnoredKeywords = [
        "CONSTRAINT",
        "FOREIGN",
        "KEY",
        "FULLTEXT",
        "INDEX",
        "UNIQUE",
        "CHECK",
        "PERIOD",
    ]
    ddlConstraint = (
        pp.Or(map(
            pp.CaselessKeyword,
            sorted(ddlIgnoredKeywords + ["PRIMARY"], reverse=True)
        ))
        + ddlExpression
    ).setResultsName("isConstraint")

    ddlColumn = pp.Group(
        ddlName.setResultsName("name")
        + ddlType.setResultsName("type")
        + pp.Suppress(pp.Optional(ddlWidth))
        + pp.Suppress(pp.Optional(ddlTimezone))
        + pp.ZeroOrMore(
            ddlUnsigned
            | ddlNotNull
            | pp.Suppress(pp.CaselessKeyword("NULL"))
            | ddlAutoValue
            | ddlDefaultValue
            | ddlGeneratedValue
            | ddlPrimaryKey
            | pp.Suppress(pp.OneOrMore(pp.Or(map(pp.CaselessKeyword, sorted(ddlIgnoredKeywords, reverse=True)))))
            | pp.Suppress(ddlExpression)
        )
    )

    # CREATE TABLE parser
    def addCreateSql(text, loc, parsed):
        parsed.create["createSql"] = text
    ddlCreateTable = (
        pp.Group(
            pp.Suppress(pp.CaselessKeyword("CREATE"))
            + pp.Suppress(pp.Optional(pp.CaselessKeyword("OR") + pp.CaselessKeyword("REPLACE")))
            + pp.Suppress(pp.CaselessKeyword("TABLE"))
            + pp.Suppress(pp.Optional(pp.CaselessKeyword("IF") + pp.CaselessKeyword("NOT") + pp.CaselessKeyword("EXISTS")))
            + ddlName.setResultsName("tableName")
            + ddlLeft
            + pp.Group(pp.delimitedList(pp.Suppress(ddlConstraint) | ddlColumn)).setResultsName("columns")
            + ddlRight
        )
        .setParseAction(addCreateSql)
        .setResultsName("create")
    )
    # ddlString.setDebug(True) #uncomment to debug pyparsing

    ddl = pp.OneOrMore(pp.Group(pp.Suppress(pp.SkipTo(ddlCreateTable, False)) + ddlCreateTable)).setResultsName("tables")

    ddlComment = pp.oneOf(["--", "#"]) + pp.restOfLine
    ddl.ignore(ddlComment)
    return ddl

def testBoolean():
    for t in ddlBooleanTypes:
        result = ddlType.parseString(t, parseAll=True)
        assert result[0] == "boolean"


def testInteger():
    for t in ddlIntegerTypes:
        result = ddlType.parseString(t, parseAll=True)
        assert result[0] == "integral"


def testSerial():
    for t in ddlSerialTypes:
        result = ddlType.parseString(t, parseAll=True)
        assert result[0] == "integral"
        assert result.hasSerialValue


def testFloatingPoint():
    for t in ddlFloatingPointTypes:
        result = ddlType.parseString(t, parseAll=True)
        assert result[0] == "floating_point"


def testText():
    for t in ddlTextTypes:
        result = ddlType.parseString(t, parseAll=True)
        assert result[0] == "text"


def testBlob():
    for t in ddlBlobTypes:
        result = ddlType.parseString(t, parseAll=True)
        assert result[0] == "blob"


def testDate():
    for t in ddlDateTypes:
        result = ddlType.parseString(t, parseAll=True)
        assert result[0] == "date"


def testDateTime():
    for t in ddlDateTimeTypes:
        result = ddlType.parseString(t, parseAll=True)
        assert result[0] == "timestamp"


def testTime():
    for t in ddlTimeTypes:
        result = ddlType.parseString(t, parseAll=True)
        assert result[0] == "time"


def testUnknown():
    for t in ["cheesecake", "blueberry"]:
        result = ddlType.parseString(t, parseAll=True)
        assert result[0] == "UNKNOWN"


def testColumn():
    testData = [
        {
            "text": "\"id\" int(8) unsigned NOT NULL DEFAULT nextval('dk_id_seq'::regclass)",
            "expected": {
                "name": "id",
                "type": "integral",
                "isUnsigned": True,
                "notNull": True,
                "hasAutoValue": False,
                "hasDefaultValue": True,
                "hasGeneratedValue": False,
                "hasSerialValue": False,
                "isPrimaryKey": False
            }
        },
        {
            "text": "\"fld\" int AUTO_INCREMENT",
            "expected": {
                "name": "fld",
                "type": "integral",
                "isUnsigned": False,
                "notNull": False,
                "hasAutoValue": True,
                "hasDefaultValue": False,
                "hasGeneratedValue": False,
                "hasSerialValue": False,
                "isPrimaryKey": False
            }
        },
        {
            "text": "\"fld2\" int NOT NULL GENERATED ALWAYS AS abc+1",
            "expected": {
                "name": "fld2",
                "type": "integral",
                "isUnsigned": False,
                "notNull": True,
                "hasAutoValue": False,
                "hasDefaultValue": False,
                "hasGeneratedValue": True,
                "hasSerialValue": False,
                "isPrimaryKey": False
            }
        }
    ]
    for td in testData:
        result = ddlColumn.parseString(td["text"], parseAll=True)[0]
        expected = td["expected"]
        assert result.name == expected["name"]
        assert result.type == expected["type"]
        assert bool(result.isUnsigned) == expected["isUnsigned"]
        assert bool(result.notNull) == expected["notNull"]
        assert bool(result.hasAutoValue) == expected["hasAutoValue"]
        assert bool(result.hasDefaultValue) == expected["hasDefaultValue"]
        assert bool(result.hasGeneratedValue) == expected["hasGeneratedValue"]
        assert bool(result.hasSerialValue) == expected["hasSerialValue"]
        assert bool(result.isPrimaryKey) == expected["isPrimaryKey"]


def testConstraint():
    for text in [
        "CONSTRAINT unique_person UNIQUE (first_name, last_name)",
        "UNIQUE (id)",
        "UNIQUE (first_name,last_name)"
    ]:
        result = ddlConstraint.parseString(text, parseAll=True)
        assert result.isConstraint

def testMathExpression():
        text = "2 DIV 2"
        result = ddlExpression.parseString(text, parseAll=True)
        assert len(result) == 3
        assert result[0] == "2"
        assert result[1] == "DIV"
        assert result[2] == "2"


def testRational():
    for text in [
        "pos RATIONAL NOT NULL DEFAULT nextval('rational_seq')::integer",
     ]:
        result = ddlColumn.parseString(text, parseAll=True)
        column = result[0]
        assert column.name == "pos"
        assert column.type == "text"
        assert column.notNull


def testTable():
    text = """
  CREATE TABLE "public"."dk" (
  "id" int8 NOT NULL DEFAULT nextval('dk_id_seq'::regclass),
  "strange" NUMERIC(314, 15),
  "last_update" timestamp(6) DEFAULT now(),
   PRIMARY KEY (id)
)
"""
    result = ddlCreateTable.parseString(text, parseAll=True)

def testPrimaryKeyAutoIncrement():
    for text in [
        "CREATE TABLE tab (col INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY)", # mysql
        "CREATE TABLE tab (col INTEGER NOT NULL PRIMARY KEY AUTO_INCREMENT)", # mysql
        "CREATE TABLE tab (col INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT)", # sqlite
    ]:
        result = ddlCreateTable.parseString(text, parseAll=True)
        assert len(result) == 1
        table = result[0]
        assert table.tableName == "tab"
        assert len(table.columns) == 1
        column = table.columns[0]
        assert not column.isConstraint
        assert column.name == "col"
        assert column.type == "integral"
        assert column.notNull
        assert column.hasAutoValue
        assert column.isPrimaryKey
        assert table.createSql == text

def testParser():
    createDdlParser()
    testBoolean()
    testInteger()
    testSerial()
    testFloatingPoint()
    testText()
    testBlob()
    testDate()
    testTime()
    testUnknown()
    testDateTime()
    testColumn()
    testConstraint()
    testMathExpression()
    testRational()
    testTable()
    testPrimaryKeyAutoIncrement()



# HELPERS
def repl_camel_case_func(m):
    if m.group(1) == "_":
        return m.group(2).upper()
    else:
        return m.group(1) + m.group(2).upper()


def loadExtendedTypesFile(filename):
    import csv
    with open(filename, newline="") as csvfile:
        reader = csv.DictReader(csvfile, fieldnames=["baseType"], restkey="extendedTypes", delimiter=",")
        for row in reader:
            var_values = [clean_val for value in row["extendedTypes"] if (clean_val := value.strip(" \"'"))]
            if var_values:
                var_name = f"ddl{row['baseType']}Types"
                globals()[var_name].extend(var_values)

def escape_if_reserved(name):
    reserved_names = [
        "BEGIN",
        "END",
        "GROUP",
        "ORDER",
    ]
    if name.upper() in reserved_names:
        return "!{}".format(name)
    return name


def beginHeader(pathToHeader, args):
    header = open(pathToHeader, "w")
    print("#pragma once", file=header)
    print("", file=header)
    print("// clang-format off", file=header)
    print("// generated by " + " ".join(sys.argv), file=header)
    print("", file=header)
    if args.use_import_std:
        print("import std;", file=header)
    else:
        print("#include <optional>", file=header)
    if args.use_import_sqlpp23:
        print("import sqlpp23.core;", file=header)
        print("", file=header)
        print("#include <sqlpp23/core/name/create_name_tag.h>", file=header)
    else:
        print("", file=header)
        print("#include <sqlpp23/core/basic/table.h>", file=header)
        print("#include <sqlpp23/core/basic/table_columns.h>", file=header)
        print("#include <sqlpp23/core/name/create_name_tag.h>", file=header)
        print("#include <sqlpp23/core/type_traits.h>", file=header)
    print("", file=header)
    print("namespace " + args.namespace + " {", file=header)
    return header


def endHeader(header, args):
    print("} // namespace " + args.namespace, file=header)
    header.close()


def beginModule(pathToModule, args):
    module = open(pathToModule, "w")
    print("module;", file=module)
    print("", file=module)
    print("// clang-format off", file=module)
    print("// generated by " + " ".join(sys.argv), file=module)
    print("", file=module)
    if args.use_import_std:
        print("import std;", file=module)
    else:
        print("#include <optional>", file=module)
    print("", file=module)
    print("#include <sqlpp23/core/name/create_name_tag.h>", file=module)
    print("", file=module)
    print("import sqlpp23.core;", file=module)
    print("", file=module)
    print("export module " + args.module_name + ";", file=module)
    print("", file=module)
    print("namespace " + args.namespace + " {", file=module)
    return module


def endModule(module, args):
    print("} // namespace " + args.namespace, file=module)
    module.close()


def parseCommandlineArgs():
    argParser = argparse.ArgumentParser(prog="sqlpp23-ddl2cpp")
    required = argParser.add_argument_group("Required parameters for code generation")
    required.add_argument("--path-to-ddl", nargs="*", help="one or more path(s) to DDL input file(s)")
    required.add_argument("--namespace", help="namespace for generated table classes")

    paths = argParser.add_argument_group("Paths", "Choose one or more paths for code generation:")
    paths.add_argument("--path-to-module", help="path to generated module file (also requires --module-name)")
    paths.add_argument("--path-to-header", help="path to generated header file (one file for all tables)")
    paths.add_argument("--path-to-header-directory", help="path to directory for generated header files (one file per table)")
    paths.add_argument("--path-to-datatype-file", help="path to csv file containing additional sql2cpp file type mappings")

    options = argParser.add_argument_group("Additional options")
    options.add_argument("--module-name", help="name of the generated module (to be used with --path-to-module)")
    options.add_argument("--suppress-timestamp-warning", action="store_true", help="suppress show warning about date / time data types")
    options.add_argument("--assume-auto-id", action="store_true", help="assume column 'id' to have an automatic value as if AUTO_INCREMENT was specified (e.g. implicit for SQLite ROWID (default: False)")
    options.add_argument("--naming-style", choices=["camel-case", "identity"], default="camel-case", help="naming style for generated tables and columns.\n\n\n\n 'camel-case' (default): interprets '_' as word separator and translates table names to UpperCamelCase and column names to lowerCamelCase, e.g. 'my_cool_table.important_column' will be represented as 'MyCoolTable{}.importantColumn' in generated code.\n 'identity' uses table and column names as is in generated code (default: 'camel-case')")
    options.add_argument("--generate-table-creation-helper", action="store_true", help="create a helper function for each table that drops and creates the table")
    options.add_argument("--use-import-sqlpp23", action="store_true", help="import sqlpp23 as module instead of including the header file (default: False)")
    options.add_argument("--use-import-std", action="store_true", help="import std as module instead of including the respective standard header files (default: False)")
    options.add_argument("--self-test", action="store_true", help="run parser self-test (this ignores all other arguments)")

    args = argParser.parse_args()

    if args.self_test:
        return args

    if not args.path_to_ddl or not len(args.path_to_ddl):
        print("Missing argument --path-to-ddl")
        argParser.print_help()
        sys.exit(ExitCode.BAD_ARGS)

    if not args.namespace:
        print("Missing argument --namespace")
        argParser.print_help()
        sys.exit(ExitCode.BAD_ARGS)

    if not args.path_to_module and not args.path_to_header and not args.path_to_header_directory:
        print("Missing argument(s): at least one path for code generation")
        argParser.print_help()
        sys.exit(ExitCode.BAD_ARGS)

    if args.path_to_module and not args.module_name:
        print("Missing argument --module-name")
        argParser.print_help()
        sys.exit(ExitCode.BAD_ARGS)

    return args

def parseDdls(ddlParser, args):
    try:
        ddls = []
        for path in args.path_to_ddl:
            ddls.append(ddlParser.parseFile(path))
        return ddls
    except pp.ParseException as e:
        print("ERROR: failed to parse " + path)
        print(e.explain(1))
        sys.exit(ExitCode.STRANGE_PARSING)

def toClassName(name, args):
    if args.naming_style == "camel-case":
        name = name.replace(".", "_")
        return re.sub(r"(^|\s|[_0-9])(\S)", repl_camel_case_func, name)
    # otherwise return identity
    return name

def toMemberName(name, args):
    if args.naming_style == "camel-case":
        name = name.replace(".", "_")
        return re.sub(r"(\s|_|[0-9])(\S)", repl_camel_case_func, name)
    # otherwise return identity
    return name

def writeTable(table, header, args):
    export = "export " if args.path_to_module else ""

    DataTypeError = False
    create = table.create
    sqlTableName = create.tableName
    tableClass = toClassName(sqlTableName, args)
    tableMember = toMemberName(sqlTableName, args)
    tableSpec = tableClass + "_"
    tableTemplateParameters = ""
    tableRequiredInsertColumns = ""
    if args.generate_table_creation_helper:
        creationHelperFunc = "create" + ("" if args.naming_style == "camel-case" else "_") + tableClass
        print("  " + export + "template<typename Db>", file=header)
        print("  void " + creationHelperFunc + "(Db& db) {", file=header)
        print("    db(R\"+++(DROP TABLE IF EXISTS " + sqlTableName + ")+++\");", file=header)
        print("    db(R\"+++(" + create.createSql + ")+++\");", file=header)
        print("  }", file=header)
        print("", file=header)
    print("  " + export + "struct " + tableSpec + " {", file=header)
    for column in create.columns:
        if column.isConstraint:
            continue
        sqlColumnName = column.name
        columnClass = toClassName(sqlColumnName, args)
        columnMember = toMemberName(sqlColumnName, args)
        columnType = column.type
        if columnType == "UNKNOWN":
            print(
                "Error: datatype of %s.%s is not supported."
                % (sqlTableName, sqlColumnName)
            )
            DataTypeError = True
        if columnType == "integral" and column.isUnsigned:
            columnType = "unsigned_" + columnType
        if columnType == "timestamp" and not args.suppress_timestamp_warning:
            args.suppress_timestamp_warning = True
            print(
                "Warning: date and time values are assumed to be without timezone."
            )
            print(
                "Warning: If you are using types WITH timezones, your code has to deal with that."
            )
            print("You can disable this warning using --suppress-timestamp-warning")
        print("    struct " + columnClass + " {", file=header)
        print("      SQLPP_CREATE_NAME_TAG_FOR_SQL_AND_CPP("
            + escape_if_reserved(sqlColumnName) + ", " + columnMember + ");"
            , file=header)
        columnIsConst = column.hasGeneratedValue
        constPrefix = "const " if columnIsConst else ""
        columnCanBeNull = not column.notNull and not column.isPrimaryKey and not column.hasSerialValue
        if columnCanBeNull:
            print("      using data_type = " + constPrefix + "std::optional<::sqlpp::" + columnType + ">;", file=header)
        else:
            print("      using data_type = " + constPrefix + "::sqlpp::" + columnType + ";", file=header)
        columnHasDefault = column.hasDefaultValue or \
                           column.hasSerialValue or \
                           column.hasAutoValue or \
                           column.hasGeneratedValue or \
                           (args.assume_auto_id and sqlColumnName == "id") or \
                           columnCanBeNull
        if columnHasDefault:
          print("      using has_default = std::true_type;", file=header)
        else:
          print("      using has_default = std::false_type;", file=header)
        print("    };", file=header)
        if tableTemplateParameters:
          tableTemplateParameters += ","
        tableTemplateParameters += "\n               " + columnClass
        if not columnHasDefault:
          if tableRequiredInsertColumns:
            tableRequiredInsertColumns += ","
          tableRequiredInsertColumns += "\n               sqlpp::column_t<sqlpp::table_t<" + tableSpec + ">, " + columnClass + ">";
    print("    SQLPP_CREATE_NAME_TAG_FOR_SQL_AND_CPP("
        + escape_if_reserved(sqlTableName) + ", " + tableMember + ");"
        , file=header)
    print("    template<typename T>", file=header)
    print("    using _table_columns = sqlpp::table_columns<T,"
        + tableTemplateParameters
        + ">;", file=header)
    print("    using _required_insert_columns = sqlpp::detail::type_set<"
        + tableRequiredInsertColumns
        + ">;", file=header)
    print("  };", file=header)
    print(
        "  " + export + "using " + tableClass + " = ::sqlpp::table_t<" + tableSpec + ">;", file=header)
    print("", file=header)

    if DataTypeError:
        print("Error: unsupported datatypes.")
        print("Possible solutions:")
        print("A) Implement this datatype (examples: sqlpp23/data_types)")
        print("B) Use the '{dataTypeFileArg}' command line argument to map the type to a known type (example: README)")
        print("C) Extend/upgrade sqlpp23-ddl2cpp (edit types map)")
        print("D) Raise an issue on github")
        sys.exit(ExitCode.BAD_DATA_TYPE)  # return non-zero error code, we might need it for automation

def createHeader(parsedDdls, args):
    header = beginHeader(args.path_to_header, args)

    for parsedDdl in parsedDdls:
        for table in parsedDdl.tables:
            writeTable(table, header, args)

    endHeader(header, args)

def createSplitHeaders(parsedDdl, args):
    for parsedDdl in parsedDdls:
        for table in parsedDdl.tables:
            create = table.value.create
            sqlTableName = create.tableName
            header = beginHeader(os.path.join(args.path_to_header_directory, toClassName(sqlTableName, args) + ".h"), args)

            writeTable(table, header, args)

            endHeader(header, args)

def createModule(parsedDdl, args):
    module = beginModule(args.path_to_module, args)

    for parsedDdl in parsedDdls:
        for table in parsedDdl.tables:
            writeTable(table, module, args)

    endModule(module, args)

if __name__ == "__main__":
    args = parseCommandlineArgs()

    if args.self_test:
        print("Running self-test")
        testParser()
    else:
        if args.path_to_datatype_file:
            loadExtendedTypesFile(args.path_to_datatype_file)
        ddlParser = createDdlParser()
        parsedDdls = parseDdls(ddlParser, args)
        if args.path_to_header:
            createHeader(parsedDdls, args)
        if args.path_to_header_directory:
            createSplitHeaders(parsedDdls, args)
        if args.path_to_module:
            createModule(parsedDdls, args)
    sys.exit(ExitCode.SUCCESS)

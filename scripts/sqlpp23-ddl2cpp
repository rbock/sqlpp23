#!/usr/bin/env python3

##
# Copyright (c) 2013, Roland Bock
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without modification,
# are permitted provided that the following conditions are met:
#
#  * Redistributions of source code must retain the above copyright notice,
#    this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright notice,
#    this list of conditions and the following disclaimer in the documentation
#    and/or other materials provided with the distribution.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
# IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,
# INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
# LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
# OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
# OF THE POSSIBILITY OF SUCH DAMAGE.
##

import argparse
import pyparsing as pp
import sys
import re
import os
from dataclasses import dataclass
from enum import IntEnum
from typing import Any, Dict, List


class ExitCode(IntEnum):
    SUCCESS = 0
    BAD_ARGS = 1
    BAD_CUSTOM_TYPES = 30
    BAD_DDL_COMMAND = 10
    BAD_DDL_PARSING = 20


# The following code could be organized as modules instead of classes. However, using modules
# implies that the program code would be split into several files and we want to avoid that
# because it would complicate program installation. That is why we use classes.


class DdlParser:
    """Rather basic SQL command parser that is not geared at fully parsing all SQL dialects or
    even the full ANSI SQL. Instead it recognizes only the SQL subset that is needed to build
    the database model, while ignoring anything else.
    """

    # Names of data types that can be updated by the custom data types
    ddl_boolean_types = [
        "BOOL",
        "BOOLEAN",
    ]
    ddl_integral_types = [
        "BIGINT",
        "INT",
        "INT2",  # PostgreSQL
        "INT4",  # PostgreSQL
        "INT8",  # PostgreSQL
        "INTEGER",
        "MEDIUMINT",
        "SMALLINT",
        "TINYINT",
    ]
    ddl_serial_types = [
        "BIGSERIAL",  # PostgreSQL
        "SERIAL",  # PostgreSQL
        "SERIAL2",  # PostgreSQL
        "SERIAL4",  # PostgreSQL
        "SERIAL8",  # PostgreSQL
        "SMALLSERIAL",  # PostgreSQL
    ]
    ddl_floating_point_types = [
        "DECIMAL",  # MYSQL
        "DOUBLE",
        "FLOAT8",  # PostgreSQL
        "FLOAT",
        "FLOAT4",  # PostgreSQL
        "NUMERIC",  # PostgreSQL
        "REAL",
    ]
    ddl_text_types = [
        "CHAR",
        "VARCHAR",
        "CHARACTER VARYING",  # PostgreSQL
        "TEXT",
        "CLOB",
        "ENUM",  # MYSQL
        "SET",
        "LONGTEXT",  # MYSQL
        "JSONB",  # PostgreSQL
        "JSON",  # PostgreSQL
        "TINYTEXT",  # MYSQL
        "MEDIUMTEXT",  # MYSQL
        "RATIONAL", # PostgreSQL pg_rationale extension
    ]
    ddl_blob_types = [
        "BYTEA",
        "TINYBLOB",
        "BLOB",
        "MEDIUMBLOB",
        "LONGBLOB",
        "BINARY",  # MYSQL
        "VARBINARY",  # MYSQL
    ]
    ddl_date_types = [
        "DATE",
    ]
    ddl_timestamp_types = [
        "DATETIME",
        "TIMESTAMP",
        "TIMESTAMP WITHOUT TIME ZONE",  # PostgreSQL
        "TIMESTAMP WITH TIME ZONE",  # PostgreSQL
        "TIMESTAMPTZ",  # PostgreSQL
    ]
    ddl_time_types = [
        "TIME",
        "TIME WITHOUT TIME ZONE",  # PostgreSQL
        "TIME WITH TIME ZONE",  # PostgreSQL
    ]

    # Parsers that are initialized later
    ddl_expression = None
    ddl_type = None
    ddl_column = None
    ddl_constraint = None
    ddl_ct_with_sql = None
    ddl = None

    @classmethod
    def initialize(cls, custom_types=None):
        """Initialize the DDL parser"""

        # Basic parsers
        ddl_left = pp.Suppress("(")
        ddl_right = pp.Suppress(")")
        ddl_number = pp.Word(pp.nums + "+-.", pp.nums + "+-.Ee")
        ddl_string = pp.QuotedString("'") | pp.QuotedString('"', esc_quote='""') | pp.QuotedString("`")
        # ddl_string.set_debug(True) #uncomment to debug pyparsing
        ddl_term = pp.Word(pp.alphas + "_", pp.alphanums + "_.$")
        ddl_name = pp.Or([ddl_term, ddl_string, pp.Combine(ddl_string + "." + ddl_string), pp.Combine(ddl_term + ddl_string)])
        ddl_operator = pp.Or(
            map(pp.CaselessLiteral, ["+", "-", "*", "/", "<", "<=", ">", ">=", "=", "%"]),
            pp.CaselessKeyword("DIV")
        )
        ddl_braced_expression = pp.Forward()
        ddl_function_call = pp.Forward()
        ddl_cast_end = "::" + ddl_term
        ddl_cast = ddl_string + ddl_cast_end
        ddl_braced_arguments = pp.Forward()
        cls.ddl_expression = pp.OneOrMore(
            ddl_braced_expression
            | ddl_function_call
            | ddl_cast_end
            | ddl_cast
            | ddl_operator
            | ddl_string
            | ddl_term
            | ddl_number
            | ddl_braced_arguments
        )
        ddl_braced_arguments << ddl_left + pp.DelimitedList(cls.ddl_expression) + ddl_right
        ddl_braced_expression << ddl_left + cls.ddl_expression + ddl_right
        ddl_arguments = pp.Suppress(pp.DelimitedList(cls.ddl_expression))
        ddl_function_call << ddl_name + ddl_left + pp.Optional(ddl_arguments) + ddl_right

        # Data type parsers
        def get_type_parser(base_type, data_type):
            type_names = getattr(cls, f"ddl_{base_type}_types")
            if custom_types and (base_type in custom_types):
                type_names.extend(custom_types[base_type])
            return pp.Or(
                map(pp.CaselessKeyword, sorted(type_names, reverse=True))
            ).set_parse_action(pp.replace_with(data_type))

        ddl_boolean = get_type_parser("boolean", "boolean")
        ddl_integral = get_type_parser("integral", "integral")
        ddl_serial = get_type_parser("serial", "integral").set_results_name("has_serial_value")
        ddl_floating_point = get_type_parser("floating_point", "floating_point")
        ddl_text = get_type_parser("text", "text")
        ddl_blob = get_type_parser("blob", "blob")
        ddl_date = get_type_parser("date", "date")
        ddl_timestamp = get_type_parser("timestamp", "timestamp")
        ddl_time = get_type_parser("time", "time")
        ddl_unknown = pp.Word(pp.alphanums).set_parse_action(pp.replace_with("UNKNOWN"))
        cls.ddl_type = (
            ddl_boolean
            | ddl_integral
            | ddl_serial
            | ddl_floating_point
            | ddl_text
            | ddl_blob
            | ddl_timestamp
            | ddl_date
            | ddl_time
            | ddl_unknown
        )

        # Constraints parser
        ddl_unsigned = pp.CaselessKeyword("UNSIGNED").set_results_name("is_unsigned")
        ddl_digits = "," + pp.Word(pp.nums)
        ddl_width = ddl_left + pp.Word(pp.nums) + pp.Optional(ddl_digits) + ddl_right
        ddl_timezone = (
            (pp.CaselessKeyword("WITH") | pp.CaselessKeyword("WITHOUT"))
            + pp.CaselessKeyword("TIME")
            + pp.CaselessKeyword("ZONE")
        )
        ddl_not_null = (pp.CaselessKeyword("NOT") + pp.CaselessKeyword("NULL")).set_results_name("not_null")
        ddl_default_value = pp.CaselessKeyword("DEFAULT").set_results_name("has_default_value")
        ddl_generated_value = pp.CaselessKeyword("GENERATED").set_results_name("has_generated_value")
        ddl_auto_keywords = [
            "AUTO_INCREMENT",
            "AUTOINCREMENT"
        ]
        ddl_auto_value = pp.Or(map(pp.CaselessKeyword, sorted(ddl_auto_keywords, reverse=True))).set_results_name("has_auto_value")
        ddl_primary_key = (pp.CaselessKeyword("PRIMARY") + pp.CaselessKeyword("KEY")).set_results_name("is_primary_key")
        ddl_ignored_keywords = [
            "CONSTRAINT",
            "FOREIGN",
            "KEY",
            "FULLTEXT",
            "INDEX",
            "UNIQUE",
            "CHECK",
            "PERIOD",
        ]
        cls.ddl_constraint = (
            pp.Or(map(
                pp.CaselessKeyword,
                sorted(ddl_ignored_keywords + ["PRIMARY"], reverse=True)
            ))
            + cls.ddl_expression
        ).set_results_name("is_constraint")

        # Column parser
        cls.ddl_column = pp.Group(
            ddl_name.set_results_name("name")
            + cls.ddl_type.set_results_name("type")
            + pp.Suppress(pp.Optional(ddl_width))
            + pp.Suppress(pp.Optional(ddl_timezone))
            + pp.ZeroOrMore(
                ddl_unsigned
                | ddl_not_null
                | pp.Suppress(pp.CaselessKeyword("NULL"))
                | ddl_auto_value
                | ddl_default_value
                | ddl_generated_value
                | ddl_primary_key
                | pp.Suppress(pp.OneOrMore(pp.Or(map(pp.CaselessKeyword, sorted(ddl_ignored_keywords, reverse=True)))))
                | pp.Suppress(cls.ddl_expression)
            )
        )

        # CREATE TABLE parser
        ddl_ct_basic = (
            pp.Suppress(pp.CaselessKeyword("CREATE"))
            + pp.Suppress(pp.Optional(pp.CaselessKeyword("OR") + pp.CaselessKeyword("REPLACE")))
            + pp.Suppress(pp.CaselessKeyword("TABLE"))
            + pp.Suppress(pp.Optional(pp.CaselessKeyword("IF") + pp.CaselessKeyword("NOT") + pp.CaselessKeyword("EXISTS")))
            + ddl_name.set_results_name("table_name")
            + ddl_left
            + pp.Group(pp.DelimitedList(pp.Suppress(cls.ddl_constraint) | cls.ddl_column)).set_results_name("columns")
            + ddl_right
        )
        def add_command_text(text, loc, tokens):
            create = tokens.create
            create.value["command_text"] = text[create.locn_start:create.locn_end]
        cls.ddl_ct_with_sql = pp.Located(ddl_ct_basic).set_results_name("create").set_parse_action(add_command_text)

        # Main DDL parser
        ddl_comment = pp.one_of(["--", "#"]) + pp.rest_of_line
        cls.ddl = (
            pp.OneOrMore(pp.Group(pp.Suppress(pp.SkipTo(ddl_ct_basic, False)) + cls.ddl_ct_with_sql))
            .set_results_name("tables")
            .ignore(ddl_comment)
            .parse_with_tabs()
        )

    @classmethod
    def is_base_type(cls, name):
        return hasattr(cls, f"ddl_{name}_types")

    @classmethod
    def parse_ddls(cls, ddl_paths):
        try:
            return [cls.ddl.parse_file(path) for path in ddl_paths]
        except pp.ParseException as e:
            print(f"ERROR: Failed to parse {path}")
            print(e.explain(1))
            sys.exit(ExitCode.BAD_DDL_PARSING)


class DdlExecutor:
    """Executes the parsed DDL commands and builds the database model"""

    @dataclass
    class _ColumnDef:
        name: str
        data_type: str
        is_const: bool
        is_nullable: bool
        has_default: bool

    @dataclass
    class _TableDef:
        name: str
        columns: Dict[str, Any]
        commands: List[str]

    _assume_auto_id = None
    _warn_on_timestamp = None
    _has_error = None
    _has_unknown_type = None
    _tables = None

    @classmethod
    def execute(cls, parsed_ddls, args):
        cls._assume_auto_id = args.assume_auto_id
        cls._warn_on_timestamp = not args.suppress_timestamp_warning
        cls._has_error = False
        cls._has_unknown_type = False
        cls._tables = {}
        for pd in parsed_ddls:
            for command in pd.tables:
                cls._execute_create_table(command)
        if cls._has_error:
            if cls._has_unknown_type:
                print("ERROR: Unsupported SQL data type(s).")
                print("Possible solutions:")
                print("A) Use the '--path-to-custom-types' command line argument to map the SQL data type to a known sqlpp23 data type (example: README)")
                print("B) Implement this data type in sqlpp23 (examples: sqlpp23/data_types) and in sqlpp23-ddl2cpp")
                print("C) Raise an issue on github")
            sys.exit(ExitCode.BAD_DDL_COMMAND)
        return cls._tables

    @classmethod
    def _execute_create_table(cls, command):
        table_expr = command.create.value
        table_name = table_expr.table_name
        if table_name in cls._tables:
            print(f"ERROR: Duplicate table name {table_name}")
            cls._has_error = True
            return
        else:
            cls._tables[table_name] = cls._TableDef(
                name = table_name,
                # We return the columns as a dictionary. Later, when we iterate the columns dictionary,
                # we rely on the column iteration order being the same as the same as the column
                # insertion order. This is only guaranteed in Python 3.7+
                columns = cls._get_create_table_columns(table_name, table_expr.columns),
                commands = [table_expr.command_text]
            )

    @classmethod
    def _get_create_table_columns(cls, table_name, cols_expr):
        columns = {}
        for ce in cols_expr:
            cls._get_create_table_column(columns, table_name, ce)
        return columns

    @classmethod
    def _get_create_table_column(cls, columns, table_name, col_expr):
        if col_expr.is_constraint:
            return
        col_name = col_expr.name
        if col_name in columns:
            print(f"ERROR: Duplicate column name {table_name}.{col_name}")
            cls._has_error = True
            return
        if col_expr.type == "timestamp":
            if cls._warn_on_timestamp:
                print("WARNING: date and time values are assumed to be without timezone.")
                print("WARNING: If you are using types WITH timezones, your code has to deal with that.")
                print("WARNING: You can disable this warning using --suppress-timestamp-warning")
                cls._warn_on_timestamp = False
        elif col_expr.type == "UNKNOWN":
            print(f"ERROR: SQL data type of {table_name}.{col_name} is not supported.")
            cls._has_error = True
            cls._has_unknown_type = True
            return
        data_type = col_expr.type
        if data_type == "integral" and col_expr.is_unsigned:
            data_type = "unsigned_" + data_type
        is_nullable = (
            not col_expr.not_null and
            not col_expr.is_primary_key and
            not col_expr.has_serial_value
        )
        columns[col_name] = cls._ColumnDef(
            name = col_name,
            data_type = data_type,
            is_const = col_expr.has_generated_value,
            is_nullable = is_nullable,
            has_default = (
                col_expr.has_default_value or
                col_expr.has_serial_value or
                col_expr.has_auto_value or
                (cls._assume_auto_id and col_name == "id") or
                col_expr.has_generated_value or
                is_nullable
            )
        )


class ModelWriter:
    """This class writes the database model as C++ headers and/or a module file"""

    @classmethod
    def write(cls, tables, args):
        if args.path_to_header:
            cls._create_header(tables, args)
        if args.path_to_header_directory:
            cls._create_split_headers(tables, args)
        if args.path_to_module:
            cls._create_module(tables, args)

    @classmethod
    def _create_header(cls, tables, args):
        header = cls._begin_header(args.path_to_header, args)
        for t in tables.values():
            cls._write_table(t, header, args)
        cls._end_header(header, args)

    @classmethod
    def _create_split_headers(cls, tables, args):
        for t in tables.values():
            header = cls._begin_header(os.path.join(args.path_to_header_directory, cls._to_class_name(t.name, args) + ".h"), args)
            cls._write_table(t, header, args)
            cls._end_header(header, args)

    @staticmethod
    def _begin_header(path_to_header, args):
        header = open(path_to_header, "w")
        print("#pragma once", file=header)
        print("", file=header)
        print("// clang-format off", file=header)
        print("// generated by " + " ".join(sys.argv), file=header)
        print("", file=header)
        if args.use_import_std:
            print("import std;", file=header)
        else:
            print("#include <optional>", file=header)
        if args.use_import_sqlpp23:
            print("import sqlpp23.core;", file=header)
            print("", file=header)
            print("#include <sqlpp23/core/name/create_name_tag.h>", file=header)
        else:
            print("", file=header)
            print("#include <sqlpp23/core/basic/table.h>", file=header)
            print("#include <sqlpp23/core/basic/table_columns.h>", file=header)
            print("#include <sqlpp23/core/name/create_name_tag.h>", file=header)
            print("#include <sqlpp23/core/type_traits.h>", file=header)
        print("", file=header)
        print("namespace " + args.namespace + " {", file=header)
        return header

    @staticmethod
    def _end_header(header, args):
        print("} // namespace " + args.namespace, file=header)
        header.close()

    @classmethod
    def _create_module(cls, tables, args):
        module = cls._begin_module(args.path_to_module, args)
        for t in tables.values():
            cls._write_table(t, module, args)
        cls._end_module(module, args)

    @staticmethod
    def _begin_module(path_to_module, args):
        module = open(path_to_module, "w")
        print("module;", file=module)
        print("", file=module)
        print("// clang-format off", file=module)
        print("// generated by " + " ".join(sys.argv), file=module)
        print("", file=module)
        if args.use_import_std:
            print("import std;", file=module)
        else:
            print("#include <optional>", file=module)
        print("", file=module)
        print("#include <sqlpp23/core/name/create_name_tag.h>", file=module)
        print("", file=module)
        print("import sqlpp23.core;", file=module)
        print("", file=module)
        print("export module " + args.module_name + ";", file=module)
        print("", file=module)
        print("namespace " + args.namespace + " {", file=module)
        return module

    @staticmethod
    def _end_module(module, args):
        print("} // namespace " + args.namespace, file=module)
        module.close()

    @classmethod
    def _write_table(cls, table, header, args):
        export = "export " if args.path_to_module else ""
        table_class = cls._to_class_name(table.name, args)
        table_member = cls._to_member_name(table.name, args)
        table_spec = table_class + "_"
        table_template_parameters = ""
        table_required_insert_columns = ""
        if args.generate_table_creation_helper:
            creation_helper_func = "create" + ("" if args.naming_style == "camel-case" else "_") + table_class
            print("  " + export + "template<typename Db>", file=header)
            print("  void " + creation_helper_func + "(Db& db) {", file=header)
            print("    db(R\"+++(DROP TABLE IF EXISTS " + table.name + ")+++\");", file=header)
            for command in table.commands:
                print("    db(R\"+++(" + command + ")+++\");", file=header)
            print("  }", file=header)
            print("", file=header)
        print("  " + export + "struct " + table_spec + " {", file=header)
        for column in table.columns.values():
            column_class = cls._to_class_name(column.name, args)
            column_member = cls._to_member_name(column.name, args)
            print("    struct " + column_class + " {", file=header)
            print("      SQLPP_CREATE_NAME_TAG_FOR_SQL_AND_CPP("
                + cls._escape_if_reserved(column.name) + ", " + column_member + ");"
                , file=header)
            const_prefix = "const " if column.is_const else ""
            if column.is_nullable:
                print("      using data_type = " + const_prefix + "std::optional<::sqlpp::" + column.data_type + ">;", file=header)
            else:
                print("      using data_type = " + const_prefix + "::sqlpp::" + column.data_type + ";", file=header)
            if column.has_default:
                print("      using has_default = std::true_type;", file=header)
            else:
                print("      using has_default = std::false_type;", file=header)
            print("    };", file=header)
            if table_template_parameters:
                table_template_parameters += ","
            table_template_parameters += "\n               " + column_class
            if not column.has_default:
                if table_required_insert_columns:
                    table_required_insert_columns += ","
                table_required_insert_columns += "\n               sqlpp::column_t<sqlpp::table_t<" + table_spec + ">, " + column_class + ">";
        print("    SQLPP_CREATE_NAME_TAG_FOR_SQL_AND_CPP("
            + cls._escape_if_reserved(table.name) + ", " + table_member + ");"
            , file=header)
        print("    template<typename T>", file=header)
        print("    using _table_columns = sqlpp::table_columns<T,"
            + table_template_parameters
            + ">;", file=header)
        print("    using _required_insert_columns = sqlpp::detail::type_set<"
            + table_required_insert_columns
            + ">;", file=header)
        print("  };", file=header)
        print(
            "  " + export + "using " + table_class + " = ::sqlpp::table_t<" + table_spec + ">;", file=header)
        print("", file=header)

    @classmethod
    def _to_class_name(cls, name, args):
        if args.naming_style == "camel-case":
            name = name.replace(".", "_")
            return re.sub(r"(^|\s|[_0-9])(\S)", cls._repl_camel_case_func, name)
        # otherwise return identity
        return name

    @classmethod
    def _to_member_name(cls, name, args):
        if args.naming_style == "camel-case":
            name = name.replace(".", "_")
            return re.sub(r"(\s|_|[0-9])(\S)", cls._repl_camel_case_func, name)
        # otherwise return identity
        return name

    @staticmethod
    def _repl_camel_case_func(m):
        if m.group(1) == "_":
            return m.group(2).upper()
        else:
            return m.group(1) + m.group(2).upper()

    @staticmethod
    def _escape_if_reserved(name):
        reserved_names = [
            "BEGIN",
            "END",
            "GROUP",
            "ORDER",
        ]
        if name.upper() in reserved_names:
            return "!{}".format(name)
        return name


class SelfTest:
    """Runs a self-test of the parser"""

    @classmethod
    def run(cls):
        print("Running self-test")
        DdlParser.initialize()
        cls._test_boolean()
        cls._test_integer()
        cls._test_serial()
        cls._test_floating_point()
        cls._test_text()
        cls._test_blob()
        cls._test_date()
        cls._test_time()
        cls._test_unknown()
        cls._test_date_time()
        cls._test_column()
        cls._test_constraint()
        cls._test_math_expression()
        cls._test_rational()
        cls._test_table()
        cls._test_primary_key_auto_increment()

    @staticmethod
    def _test_boolean():
        for t in DdlParser.ddl_boolean_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "boolean"

    @staticmethod
    def _test_integer():
        for t in DdlParser.ddl_integral_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "integral"

    @staticmethod
    def _test_serial():
        for t in DdlParser.ddl_serial_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "integral"
            assert result.has_serial_value

    @staticmethod
    def _test_floating_point():
        for t in DdlParser.ddl_floating_point_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "floating_point"

    @staticmethod
    def _test_text():
        for t in DdlParser.ddl_text_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "text"

    @staticmethod
    def _test_blob():
        for t in DdlParser.ddl_blob_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "blob"

    @staticmethod
    def _test_date():
        for t in DdlParser.ddl_date_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "date"

    @staticmethod
    def _test_date_time():
        for t in DdlParser.ddl_timestamp_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "timestamp"

    @staticmethod
    def _test_time():
        for t in DdlParser.ddl_time_types:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "time"

    @staticmethod
    def _test_unknown():
        for t in ["cheesecake", "blueberry"]:
            result = DdlParser.ddl_type.parse_string(t, parse_all=True)
            assert result[0] == "UNKNOWN"

    @staticmethod
    def _test_column():
        test_data = [
            {
                "text": "\"id\" int(8) unsigned NOT NULL DEFAULT nextval('dk_id_seq'::regclass)",
                "expected": {
                    "name": "id",
                    "type": "integral",
                    "is_unsigned": True,
                    "not_null": True,
                    "has_auto_value": False,
                    "has_default_value": True,
                    "has_generated_value": False,
                    "has_serial_value": False,
                    "is_primary_key": False
                }
            },
            {
                "text": "\"fld\" int AUTO_INCREMENT",
                "expected": {
                    "name": "fld",
                    "type": "integral",
                    "is_unsigned": False,
                    "not_null": False,
                    "has_auto_value": True,
                    "has_default_value": False,
                    "has_generated_value": False,
                    "has_serial_value": False,
                    "is_primary_key": False
                }
            },
            {
                "text": "\"fld2\" int NOT NULL GENERATED ALWAYS AS abc+1",
                "expected": {
                    "name": "fld2",
                    "type": "integral",
                    "is_unsigned": False,
                    "not_null": True,
                    "has_auto_value": False,
                    "has_default_value": False,
                    "has_generated_value": True,
                    "has_serial_value": False,
                    "is_primary_key": False
                }
            }
        ]
        for td in test_data:
            result = DdlParser.ddl_column.parse_string(td["text"], parse_all=True)[0]
            expected = td["expected"]
            assert result.name == expected["name"]
            assert result.type == expected["type"]
            assert bool(result.is_unsigned) == expected["is_unsigned"]
            assert bool(result.not_null) == expected["not_null"]
            assert bool(result.has_auto_value) == expected["has_auto_value"]
            assert bool(result.has_default_value) == expected["has_default_value"]
            assert bool(result.has_generated_value) == expected["has_generated_value"]
            assert bool(result.has_serial_value) == expected["has_serial_value"]
            assert bool(result.is_primary_key) == expected["is_primary_key"]

    @staticmethod
    def _test_constraint():
        for text in [
            "CONSTRAINT unique_person UNIQUE (first_name, last_name)",
            "UNIQUE (id)",
            "UNIQUE (first_name,last_name)"
        ]:
            result = DdlParser.ddl_constraint.parse_string(text, parse_all=True)
            assert result.is_constraint

    @staticmethod
    def _test_math_expression():
            text = "2 DIV 2"
            result = DdlParser.ddl_expression.parse_string(text, parse_all=True)
            assert len(result) == 3
            assert result[0] == "2"
            assert result[1] == "DIV"
            assert result[2] == "2"

    @staticmethod
    def _test_rational():
        for text in [
            "pos RATIONAL NOT NULL DEFAULT nextval('rational_seq')::integer",
        ]:
            result = DdlParser.ddl_column.parse_string(text, parse_all=True)
            column = result[0]
            assert column.name == "pos"
            assert column.type == "text"
            assert column.not_null

    @staticmethod
    def _test_table():
        text = """
    CREATE TABLE "public"."dk" (
    "id" int8 NOT NULL DEFAULT nextval('dk_id_seq'::regclass),
    "strange" NUMERIC(314, 15),
    "last_update" timestamp(6) DEFAULT now(),
    PRIMARY KEY (id)
    )
    """
        result = DdlParser.ddl_ct_with_sql.parse_string(text, parse_all=True)

    @staticmethod
    def _test_primary_key_auto_increment():
        for text in [
            "CREATE TABLE tab (col INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY)", # mysql
            "CREATE TABLE tab (col INTEGER NOT NULL PRIMARY KEY AUTO_INCREMENT)", # mysql
            "CREATE TABLE tab (col INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT)", # sqlite
        ]:
            result = DdlParser.ddl_ct_with_sql.parse_string(text, parse_all=True)
            assert len(result) == 1
            table = result.create.value
            assert table.table_name == "tab"
            assert len(table.columns) == 1
            column = table.columns[0]
            assert not column.is_constraint
            assert column.name == "col"
            assert column.type == "integral"
            assert column.not_null
            assert column.has_auto_value
            assert column.is_primary_key
            assert table.command_text == text


def parse_commandline_args():
    arg_parser = argparse.ArgumentParser(prog="sqlpp23-ddl2cpp")
    required = arg_parser.add_argument_group("Required parameters for code generation")
    required.add_argument("--path-to-ddl", nargs="*", help="one or more path(s) to DDL input file(s)")
    required.add_argument("--namespace", help="namespace for generated table classes")

    paths = arg_parser.add_argument_group("Paths", "Choose one or more paths for code generation:")
    paths.add_argument("--path-to-module", help="path to generated module file (also requires --module-name)")
    paths.add_argument("--path-to-header", help="path to generated header file (one file for all tables)")
    paths.add_argument("--path-to-header-directory", help="path to directory for generated header files (one file per table)")
    paths.add_argument("--path-to-custom-types", help="path to csv file defining aliases of existing SQL data types")

    options = arg_parser.add_argument_group("Additional options")
    options.add_argument("--module-name", help="name of the generated module (to be used with --path-to-module)")
    options.add_argument("--suppress-timestamp-warning", action="store_true", help="suppress show warning about date / time data types")
    options.add_argument("--assume-auto-id", action="store_true", help="assume column 'id' to have an automatic value as if AUTO_INCREMENT was specified (e.g. implicit for SQLite ROWID (default: False)")
    options.add_argument("--naming-style", choices=["camel-case", "identity"], default="camel-case", help="naming style for generated tables and columns.\n\n\n\n 'camel-case' (default): interprets '_' as word separator and translates table names to UpperCamelCase and column names to lowerCamelCase, e.g. 'my_cool_table.important_column' will be represented as 'MyCoolTable{}.importantColumn' in generated code.\n 'identity' uses table and column names as is in generated code (default: 'camel-case')")
    options.add_argument("--generate-table-creation-helper", action="store_true", help="create a helper function for each table that drops and creates the table")
    options.add_argument("--use-import-sqlpp23", action="store_true", help="import sqlpp23 as module instead of including the header file (default: False)")
    options.add_argument("--use-import-std", action="store_true", help="import std as module instead of including the respective standard header files (default: False)")
    options.add_argument("--self-test", action="store_true", help="run parser self-test (this ignores all other arguments)")

    args = arg_parser.parse_args()

    if args.self_test:
        return args

    if not args.path_to_ddl or not len(args.path_to_ddl):
        print("Missing argument --path-to-ddl")
        arg_parser.print_help()
        sys.exit(ExitCode.BAD_ARGS)

    if not args.namespace:
        print("Missing argument --namespace")
        arg_parser.print_help()
        sys.exit(ExitCode.BAD_ARGS)

    if not args.path_to_module and not args.path_to_header and not args.path_to_header_directory:
        print("Missing argument(s): at least one path for code generation")
        arg_parser.print_help()
        sys.exit(ExitCode.BAD_ARGS)

    if args.path_to_module and not args.module_name:
        print("Missing argument --module-name")
        arg_parser.print_help()
        sys.exit(ExitCode.BAD_ARGS)

    return args


def get_custom_types(filename):
    if not filename:
        return None
    import csv
    with open(filename, newline="") as csv_file:
        reader = csv.DictReader(csv_file, fieldnames=["base_type"], restkey="custom_types", delimiter=",")
        types = {}
        def strip_garbage(name):
            return name.strip(" \"'")
        def clean_custom_type(name):
            return strip_garbage(name).upper()
        def clean_base_type(name):
            name_ident = strip_garbage(name)
            if DdlParser.is_base_type(name_ident):
                return name_ident
            name_from_camel = re.sub(r"[A-Z]", lambda m : ("_" if m.start() else "") + m[0].lower(), name_ident)
            if DdlParser.is_base_type(name_from_camel):
                return name_from_camel
            print(f"ERROR: Custom types file uses an unknown base type {name_ident}")
            sys.exit(ExitCode.BAD_CUSTOM_TYPES)
        for row in reader:
            values = [cleaned for value in row["custom_types"] if (cleaned := clean_custom_type(value)) != ""]
            if values:
                key = clean_base_type(row["base_type"])
                types[key] = values
        return types


if __name__ == "__main__":
    args = parse_commandline_args()

    if args.self_test:
        SelfTest.run()
    else:
        custom_types = get_custom_types(args.path_to_custom_types)
        DdlParser.initialize(custom_types)
        parsed_ddls = DdlParser.parse_ddls(args.path_to_ddl)
        tables = DdlExecutor.execute(parsed_ddls, args)
        ModelWriter.write(tables, args)
    sys.exit(ExitCode.SUCCESS)
